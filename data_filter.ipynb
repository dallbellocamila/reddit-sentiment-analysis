{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db9da6c-1e0e-4d85-93fe-12bcd6fa32d5",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a68b94-0901-4e94-9a99-38f9c5e10bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740b3445-ba8f-48fb-88b3-53e6bc2194b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76e330a-f531-4c8e-8520-be51a49574fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts_titles(data):\n",
    "    posts = dict()\n",
    "\n",
    "    for k in data.keys():\n",
    "        state_posts = []\n",
    "        for p in data[k]:\n",
    "            post = {\n",
    "                \"selftext\": p[\"selftext\"],\n",
    "                \"title\": p[\"title\"]\n",
    "            }\n",
    "            state_posts.append(post)\n",
    "        posts[k] = state_posts\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334b21df-1517-4771-8a48-a13a48da18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(data):\n",
    "    comments = dict()\n",
    "\n",
    "    for k in data.keys():\n",
    "        state = data[k]\n",
    "        state_comments = []\n",
    "        \n",
    "        for p in state.keys():\n",
    "            for c in state[p]:\n",
    "                comment = {\n",
    "                    \"body\": c[\"body\"]\n",
    "                }\n",
    "                state_comments.append(comment)\n",
    "        \n",
    "        comments[k] = state_comments\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7979300-58be-47d0-8799-cb0a89d48bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw data for after and before election\n",
    "raw_after_posts = extract_data('data/after_election_posts_data.json')\n",
    "raw_after_comments = extract_data('data/after_election_SAMPLE_comments_data.json')\n",
    "raw_before_posts = extract_data('data/before_election_posts_data.json')\n",
    "raw_before_comments = extract_data('data/before_election_SAMPLE_comments_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e7cc2d-3ccb-4ade-bbd6-8667d9e8ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract just the useful data from both posts and comments\n",
    "after_posts = get_posts_titles(raw_after_posts)\n",
    "before_posts = get_posts_titles(raw_before_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18635263-aadd-4a22-8bd8-9c080066912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_comments = get_comments(raw_after_comments)\n",
    "before_comments = get_comments(raw_before_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0bd0e4-6b30-456d-ad81-e16374bc77bc",
   "metadata": {},
   "source": [
    "## Filter political posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327ad669-a2d9-4f79-8626-bfb6747649c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words to look for when filtering political posts\n",
    "keywords = [\n",
    "    \"election\",\n",
    "    \"president\",\n",
    "    \"presidential\",\n",
    "    \"vote \", # take this out for topic modeling, put in for pure filtering\n",
    "    \"voting\",\n",
    "    \"abortion\",\n",
    "    \"democracy\",\n",
    "    \"immigration\",\n",
    "    \"economy\",\n",
    "    \"war \",\n",
    "    \"ukraine\",\n",
    "    \"israel\",\n",
    "    \"palestine\",\n",
    "    \"climate\",\n",
    "    \"healthcare\",\n",
    "    \"inflation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fdb8311-f9e8-4ec2-9389-cb1e1ae5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for political posts\n",
    "def filter_political_posts(all_posts):\n",
    "    political_posts = dict()\n",
    "    \n",
    "    for k in all_posts.keys():\n",
    "        posts = []\n",
    "    \n",
    "        for post in all_posts[k]:\n",
    "            for w in keywords:\n",
    "                if w in post[\"selftext\"].lower() or w in post[\"title\"].lower():\n",
    "                    posts.append(post)\n",
    "    \n",
    "        political_posts[k] = posts\n",
    "    return political_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2978a6d-b76a-4b95-bd48-c17c314ba066",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_political_posts = filter_political_posts(after_posts)\n",
    "before_political_posts = filter_political_posts(before_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "051772ac-16e1-4fe7-87ea-4e9ddf163f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out size of political posts per state - for testing\n",
    "def count_political_posts(political_posts):\n",
    "    pol_posts_size = dict()\n",
    "    for k in political_posts.keys():\n",
    "        pol_posts_size[k] = len(political_posts[k])\n",
    "    return pol_posts_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3197339a-245e-444e-9fbf-4b77155f88e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'california': 18,\n",
       " 'michigan': 50,\n",
       " 'colorado': 3,\n",
       " 'oregon': 51,\n",
       " 'hawaii': 14,\n",
       " 'oklahoma': 24,\n",
       " 'maryland': 42,\n",
       " 'arizona': 8,\n",
       " 'virginia': 99,\n",
       " 'maine': 63,\n",
       " 'indiana': 41,\n",
       " 'iowa': 98,\n",
       " 'washington': 31,\n",
       " 'newhampshire': 38,\n",
       " 'alaska': 41,\n",
       " 'louisiana': 31,\n",
       " 'vermont': 42,\n",
       " 'newyork': 2,\n",
       " 'arkansas': 4,\n",
       " 'alabama': 7,\n",
       " 'kentucky': 12,\n",
       " 'southcarolina': 2,\n",
       " 'georgia': 38,\n",
       " 'montana': 4,\n",
       " 'delaware': 14,\n",
       " 'utah': 26,\n",
       " 'rhodeisland': 20,\n",
       " 'missouri': 123,\n",
       " 'tennessee': 2,\n",
       " 'nebraska': 29,\n",
       " 'illinois': 45,\n",
       " 'westvirginia': 11,\n",
       " 'newmexico': 33,\n",
       " 'mississippi': 17,\n",
       " 'kansas': 36,\n",
       " 'northdakota': 6,\n",
       " 'idaho': 39,\n",
       " 'southdakota': 19,\n",
       " 'wyoming': 8,\n",
       " 'nevada': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_political_posts(after_political_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "393b3025-6d26-4c90-9be3-74889ddb2e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'california': 33,\n",
       " 'michigan': 183,\n",
       " 'colorado': 9,\n",
       " 'oregon': 83,\n",
       " 'hawaii': 29,\n",
       " 'oklahoma': 67,\n",
       " 'maryland': 133,\n",
       " 'arizona': 11,\n",
       " 'virginia': 101,\n",
       " 'maine': 64,\n",
       " 'indiana': 80,\n",
       " 'iowa': 165,\n",
       " 'washington': 65,\n",
       " 'newhampshire': 88,\n",
       " 'alaska': 68,\n",
       " 'louisiana': 69,\n",
       " 'vermont': 45,\n",
       " 'newyork': 14,\n",
       " 'arkansas': 16,\n",
       " 'alabama': 26,\n",
       " 'kentucky': 4,\n",
       " 'southcarolina': 54,\n",
       " 'georgia': 196,\n",
       " 'montana': 4,\n",
       " 'delaware': 59,\n",
       " 'utah': 73,\n",
       " 'rhodeisland': 36,\n",
       " 'missouri': 376,\n",
       " 'tennessee': 25,\n",
       " 'nebraska': 129,\n",
       " 'illinois': 61,\n",
       " 'westvirginia': 41,\n",
       " 'newmexico': 41,\n",
       " 'mississippi': 22,\n",
       " 'kansas': 82,\n",
       " 'northdakota': 17,\n",
       " 'idaho': 93,\n",
       " 'southdakota': 54,\n",
       " 'wyoming': 10,\n",
       " 'nevada': 12}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_political_posts(before_political_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd16831-f088-463d-b240-8fb9370bd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for political comments\n",
    "def filter_political_comments(all_comments):\n",
    "    political_comments = dict()\n",
    "    \n",
    "    for k in all_comments.keys():\n",
    "        comments = []\n",
    "        for c in all_comments[k]:\n",
    "           for w in keywords:\n",
    "               if w in c[\"body\"]:\n",
    "                   comments.append(c)\n",
    "        political_comments[k] = comments\n",
    "    return political_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ce16701-41dc-40b5-8a7f-b5d5f397d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_political_comments = filter_political_comments(after_comments)\n",
    "before_political_comments = filter_political_comments(before_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f03256cc-97bc-4183-9f6e-66cfe8c020fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nevada': 593, 'wyoming': 198}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_political_posts(after_political_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b48f7e9-d443-4152-9549-cafbe86faadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nevada': 459, 'wyoming': 317}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_political_posts(before_political_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49a20e-90fd-478b-af15-b0736b9d79eb",
   "metadata": {},
   "source": [
    "## Filter posts by candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a1c6c97-5a92-47d7-90ce-128c3f57456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_keywords = {\n",
    "    \"trump\": [\"trump\", \"donald\", \"donald trump\", \"republican\"],\n",
    "    \"harris\": [\"harris\", \"kamala\", \"kamala harris\", \"democrat\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eb88c1c-b4f0-4698-8444-16bed735b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for candidate posts\n",
    "def filter_candidate_posts(all_posts):\n",
    "    candidate_posts = dict()\n",
    "    \n",
    "    for k in all_posts.keys():\n",
    "        candidate_posts[k] = dict()\n",
    "        \n",
    "        for candidate in candidate_keywords.keys():\n",
    "            for w in candidate_keywords[candidate]:\n",
    "                posts = set()\n",
    "                \n",
    "                for post in all_posts[k]:\n",
    "                    if w in post[\"selftext\"].lower() or w in post[\"title\"].lower():\n",
    "                        posts.add(post[\"selftext\"] + \" \" + post[\"title\"])\n",
    "        \n",
    "            candidate_posts[k][candidate] = list(posts)\n",
    "    return candidate_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "879e74f9-78eb-4f46-833b-c7d1945b621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_candidate_posts = filter_candidate_posts(after_posts)\n",
    "before_candidate_posts = filter_candidate_posts(before_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4dd9be5-f29d-4dce-98d3-c4ba9900f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out size of political posts per state - for testing\n",
    "def count_candidate_posts(candidate_posts):\n",
    "    can_posts_size = dict()\n",
    "    for k in candidate_posts.keys():\n",
    "        can_posts_size[k] = dict()\n",
    "        for candidate in candidate_keywords.keys():\n",
    "            can_posts_size[k][candidate] = len(candidate_posts[k][candidate])\n",
    "    return can_posts_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85352359-2d9b-4061-ab22-0f431ffe10e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'california': {'trump': 2, 'harris': 2},\n",
       " 'michigan': {'trump': 13, 'harris': 12},\n",
       " 'colorado': {'trump': 0, 'harris': 1},\n",
       " 'oregon': {'trump': 2, 'harris': 3},\n",
       " 'hawaii': {'trump': 3, 'harris': 0},\n",
       " 'oklahoma': {'trump': 1, 'harris': 1},\n",
       " 'maryland': {'trump': 1, 'harris': 5},\n",
       " 'arizona': {'trump': 1, 'harris': 0},\n",
       " 'virginia': {'trump': 8, 'harris': 19},\n",
       " 'maine': {'trump': 4, 'harris': 6},\n",
       " 'indiana': {'trump': 4, 'harris': 2},\n",
       " 'iowa': {'trump': 11, 'harris': 13},\n",
       " 'washington': {'trump': 3, 'harris': 5},\n",
       " 'newhampshire': {'trump': 9, 'harris': 8},\n",
       " 'alaska': {'trump': 5, 'harris': 5},\n",
       " 'louisiana': {'trump': 4, 'harris': 4},\n",
       " 'vermont': {'trump': 6, 'harris': 7},\n",
       " 'newyork': {'trump': 1, 'harris': 0},\n",
       " 'arkansas': {'trump': 0, 'harris': 1},\n",
       " 'alabama': {'trump': 1, 'harris': 0},\n",
       " 'kentucky': {'trump': 2, 'harris': 2},\n",
       " 'southcarolina': {'trump': 1, 'harris': 0},\n",
       " 'georgia': {'trump': 0, 'harris': 2},\n",
       " 'montana': {'trump': 0, 'harris': 1},\n",
       " 'delaware': {'trump': 4, 'harris': 3},\n",
       " 'utah': {'trump': 2, 'harris': 1},\n",
       " 'rhodeisland': {'trump': 3, 'harris': 1},\n",
       " 'missouri': {'trump': 14, 'harris': 14},\n",
       " 'tennessee': {'trump': 0, 'harris': 0},\n",
       " 'nebraska': {'trump': 5, 'harris': 3},\n",
       " 'illinois': {'trump': 3, 'harris': 8},\n",
       " 'westvirginia': {'trump': 0, 'harris': 2},\n",
       " 'newmexico': {'trump': 1, 'harris': 6},\n",
       " 'mississippi': {'trump': 2, 'harris': 2},\n",
       " 'kansas': {'trump': 8, 'harris': 4},\n",
       " 'northdakota': {'trump': 2, 'harris': 3},\n",
       " 'idaho': {'trump': 13, 'harris': 7},\n",
       " 'southdakota': {'trump': 2, 'harris': 1},\n",
       " 'wyoming': {'trump': 1, 'harris': 0},\n",
       " 'nevada': {'trump': 0, 'harris': 1}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_candidate_posts(after_candidate_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d362a2-2385-4173-be0b-f61bcee45e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'california': {'trump': 0, 'harris': 1},\n",
       " 'michigan': {'trump': 13, 'harris': 12},\n",
       " 'colorado': {'trump': 0, 'harris': 0},\n",
       " 'oregon': {'trump': 2, 'harris': 3},\n",
       " 'hawaii': {'trump': 0, 'harris': 0},\n",
       " 'oklahoma': {'trump': 5, 'harris': 3},\n",
       " 'maryland': {'trump': 7, 'harris': 9},\n",
       " 'arizona': {'trump': 0, 'harris': 0},\n",
       " 'virginia': {'trump': 7, 'harris': 6},\n",
       " 'maine': {'trump': 2, 'harris': 3},\n",
       " 'indiana': {'trump': 11, 'harris': 6},\n",
       " 'iowa': {'trump': 14, 'harris': 19},\n",
       " 'washington': {'trump': 5, 'harris': 3},\n",
       " 'newhampshire': {'trump': 10, 'harris': 6},\n",
       " 'alaska': {'trump': 6, 'harris': 5},\n",
       " 'louisiana': {'trump': 4, 'harris': 2},\n",
       " 'vermont': {'trump': 5, 'harris': 4},\n",
       " 'newyork': {'trump': 0, 'harris': 0},\n",
       " 'arkansas': {'trump': 0, 'harris': 1},\n",
       " 'alabama': {'trump': 1, 'harris': 1},\n",
       " 'kentucky': {'trump': 0, 'harris': 0},\n",
       " 'southcarolina': {'trump': 3, 'harris': 2},\n",
       " 'georgia': {'trump': 13, 'harris': 10},\n",
       " 'montana': {'trump': 0, 'harris': 0},\n",
       " 'delaware': {'trump': 1, 'harris': 7},\n",
       " 'utah': {'trump': 11, 'harris': 3},\n",
       " 'rhodeisland': {'trump': 3, 'harris': 4},\n",
       " 'missouri': {'trump': 33, 'harris': 22},\n",
       " 'tennessee': {'trump': 0, 'harris': 0},\n",
       " 'nebraska': {'trump': 13, 'harris': 7},\n",
       " 'illinois': {'trump': 4, 'harris': 1},\n",
       " 'westvirginia': {'trump': 3, 'harris': 1},\n",
       " 'newmexico': {'trump': 3, 'harris': 4},\n",
       " 'mississippi': {'trump': 0, 'harris': 0},\n",
       " 'kansas': {'trump': 12, 'harris': 8},\n",
       " 'northdakota': {'trump': 2, 'harris': 1},\n",
       " 'idaho': {'trump': 14, 'harris': 8},\n",
       " 'southdakota': {'trump': 5, 'harris': 2},\n",
       " 'wyoming': {'trump': 0, 'harris': 0},\n",
       " 'nevada': {'trump': 0, 'harris': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_candidate_posts(before_candidate_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85277140-2603-468d-a8b0-dc68c6a3bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for candidate comments\n",
    "def filter_candidate_comments(all_comments):\n",
    "    candidate_comments = dict()\n",
    "    \n",
    "    for k in all_comments.keys():\n",
    "        candidate_comments[k] = dict()\n",
    "        \n",
    "        for candidate in candidate_keywords.keys():\n",
    "            for w in candidate_keywords[candidate]:\n",
    "                comments = set()\n",
    "                \n",
    "                for post in all_comments[k]:\n",
    "                    if w in post[\"body\"].lower():\n",
    "                        comments.add(post[\"body\"])\n",
    "        \n",
    "            candidate_comments[k][candidate] = list(comments)\n",
    "    return candidate_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "538d35dd-1812-45e2-95ed-e5cb0e78a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_candidate_comments = filter_candidate_comments(after_comments)\n",
    "before_candidate_comments = filter_candidate_comments(before_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b4238d-a76a-49d7-aa44-5a12769fd283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nevada': {'trump': 88, 'harris': 101},\n",
       " 'wyoming': {'trump': 27, 'harris': 13}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_candidate_posts(after_candidate_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19845c76-493d-411c-912c-8566a4fc3b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nevada': {'trump': 82, 'harris': 84}, 'wyoming': {'trump': 64, 'harris': 34}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_candidate_posts(before_candidate_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42ecbf37-cfaf-4dd8-a2e5-6b4e685ad14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA FORMAT\n",
    "# <after/before>_political_posts contains posts that contain at least one of our defined keywords,\n",
    "# after or before the election respectively.\n",
    "\n",
    "# <after/before>_political_posts = {\n",
    "#    'texas': [\n",
    "#        {\n",
    "#            'selftext': \"__\",\n",
    "#            'title': \"__\"\n",
    "#        },\n",
    "#        {\n",
    "#            'selftext': \"__\",\n",
    "#            'title': \"__\"}\n",
    "#        ...\n",
    "#    ],\n",
    "#    'california': [\n",
    "#        {\n",
    "#            'selftext': \"__\",\n",
    "#            'title': \"__\"\n",
    "#        },\n",
    "#        ...\n",
    "#    ]\n",
    "# }\n",
    "\n",
    "# <after/before>_political_comments contains comments that contain at least one of our defined keywords,\n",
    "# after or before the election respectively.\n",
    "\n",
    "# <after/before>_political_posts = {\n",
    "#    'texas': [\n",
    "#        {\n",
    "#            'body': \"__\"\n",
    "#        },\n",
    "#        {\n",
    "#            'body': \"__\"\n",
    "#        },\n",
    "#        ...\n",
    "#    ],\n",
    "#    'california': [\n",
    "#        {\n",
    "#            'body': \"__\"\n",
    "#        },\n",
    "#        ...\n",
    "#    ]\n",
    "# }\n",
    "\n",
    "# <after/before>_candidate_<posts/comments> contains posts/comments that contain words about each candidate (as defined in candidate_keywords),\n",
    "# after or before the election respectively.\n",
    "\n",
    "# <after/before>_candidate_<posts/comments> = {\n",
    "#    'texas': {\n",
    "#        'trump': [__, __],\n",
    "#        'harris': [__, __],\n",
    "#    },\n",
    "#    'california': {\n",
    "#        'trump': [__, __],\n",
    "#        'harris': [__, __],\n",
    "#    },\n",
    "#    ...\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5f20-cdc2-4cbb-b312-4eea5114f19f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff077ff7-79a8-4681-9b0a-cb735a3af526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8fafc40-1503-4031-afe8-d3e271c9c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bd4650f-9c5e-433d-b03f-873881a2a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 23:38:37.212903: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49c162ce-7ec9-4ecb-aef7-19846892f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0]\n",
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n",
      "1) positive 0.9749\n",
      "2) neutral 0.0208\n",
      "3) negative 0.0043\n"
     ]
    }
   ],
   "source": [
    "## TEST - must use PyTorch version, the tensorflow one is not as accurate\n",
    "text = \"I love you!\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "print(ranking)\n",
    "print(config.id2label)\n",
    "\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e0fb060-1896-49d3-8c73-2b45c51b8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute highest sentiment score and label for given text\n",
    "def get_sentiment_label_score(text):\n",
    "    # get output from model\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    output = model(**encoded_input)\n",
    "\n",
    "    # compute softmax scores\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    # ranking = list of labels in decreasing order of sentiment score\n",
    "    ranking = np.argsort(scores)[::-1]\n",
    "\n",
    "    # return dictionary of {ranking: score} for given text\n",
    "    # sentiment_map = dict()\n",
    "    # for i in range(scores.shape[0]):\n",
    "    #     l = config.id2label[ranking[i]]\n",
    "    #     s = scores[ranking[i]]\n",
    "    #     sentiment_map[l] = s\n",
    "\n",
    "    # return largest score and the associated sentiment\n",
    "    l = config.id2label[ranking[0]]\n",
    "    s = scores[ranking[0]]\n",
    "    return l, s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc9bdc-392d-4246-9e5c-c33648cd6cae",
   "metadata": {},
   "source": [
    "## Take sentiment score for each candidate\n",
    "\n",
    "Get sentiment score for each sentiment. Might look like:\n",
    "```\n",
    "'texas': {\n",
    "    'trump': {\n",
    "        'positive': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       },\n",
    "       'neutral': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       }\n",
    "       'negative': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       },\n",
    "       'sentiment': 'positive',\n",
    "       'avg_sentiment': 0.0\n",
    "    },\n",
    "    'harris': {\n",
    "        'positive': {\n",
    "            ...\n",
    "       },\n",
    "       'neutral': {\n",
    "           ...\n",
    "       }\n",
    "       'negative': {\n",
    "           ...\n",
    "       },\n",
    "       'sentiment': 'positive',\n",
    "       'avg_sentiment': 0.0\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99c9d7f0-506d-4ac7-9661-bde0772d19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for creating the json above\n",
    "def record_sentiment(sent_data, score):\n",
    "    if \"scores\" in sent_data.keys():\n",
    "        sent_data[\"scores\"].append(score)\n",
    "    else:\n",
    "        sent_data[\"scores\"] = [score]\n",
    "\n",
    "    if \"num_posts\" in sent_data.keys():\n",
    "        sent_data[\"num_posts\"] += 1\n",
    "    else:\n",
    "        sent_data[\"num_posts\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "37daf9af-2d5a-4b6b-82e7-7f783a2906bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores_grouped_posts(grouped_posts):\n",
    "    all_sentiment = dict()\n",
    "    \n",
    "    for k in grouped_posts.keys():\n",
    "        all_sentiment[k] = dict()\n",
    "        \n",
    "        for candidate in grouped_posts[k].keys():\n",
    "            all_sentiment[k][candidate] = dict()\n",
    "            candidate_data = {\n",
    "                'positive': {},\n",
    "                'neutral': {},\n",
    "                'negative': {},\n",
    "            }\n",
    "\n",
    "            for post in grouped_posts[k][candidate]:\n",
    "                label, score = get_sentiment_label_score(post)\n",
    "                record_sentiment(candidate_data[label], score)\n",
    "                \n",
    "                all_sentiment[k][candidate] = candidate_data\n",
    "    return all_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "782c98ba-95c7-4e3f-b408-1c333dfc1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_cand_sentiments = get_sentiment_scores_grouped_posts(after_candidate_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "30251b44-e39f-401d-9547-c33b0d4e2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before_cand_sentiments = get_sentiment_scores_grouped_posts(before_candidate_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d0869024-44d2-441f-b581-0f1ec4d68000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_cand_comment_sentiments = get_sentiment_scores_grouped_posts(after_candidate_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "406779d3-37a5-43c0-855f-ed4e008c4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before_cand_comment_sentiments = get_sentiment_scores_grouped_posts(before_candidate_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5e30f0f2-0225-4164-a08e-72321368d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to make numpy types JSON serializable\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def save_data(filename, data):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, cls=NumpyEncoder, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "adb6f102-0607-4cb5-ba73-2c6d9b8f0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data('after_cand_post_sentiments.json', after_cand_sentiments)\n",
    "# save_data('before_cand_post_sentiments.json', before_cand_sentiments)\n",
    "# save_data('after_cand_comment_sentiments.json', after_cand_comment_sentiments)\n",
    "# save_data('before_cand_comment_sentiments.json', before_cand_comment_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a928ce51-0eac-418f-a970-58acfabbfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_cand_sentiments = extract_data('after_cand_post_sentiments.json')\n",
    "before_cand_sentiments = extract_data('before_cand_post_sentiments.json')\n",
    "after_cand_comment_sentiments = extract_data('after_cand_comment_sentiments.json')\n",
    "before_cand_comment_sentiments = extract_data('before_cand_comment_sentiments.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff0eb0da-e8dd-444e-bdd7-a928870d0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average, min, max, and dominant sentiment for each state\n",
    "def get_sentiment_stats(all_sentiment):\n",
    "    stats = all_sentiment.copy()\n",
    "    \n",
    "    for k in stats.keys():\n",
    "        state_sent = stats[k].copy()\n",
    "        \n",
    "        for c in state_sent.keys():\n",
    "            avgs = dict()\n",
    "            candidate = state_sent[c].copy()\n",
    "\n",
    "            for s in candidate.keys():\n",
    "                sent = candidate[s].copy()\n",
    "                if 'scores' not in sent.keys():\n",
    "                    sent['scores'] = [0]\n",
    "                    \n",
    "                sent['min'] = min(sent['scores'])\n",
    "                sent['max'] = max(sent['scores'])\n",
    "                sent['average'] = np.mean(sent['scores'])\n",
    "                avgs[sent['average']] = s\n",
    "                \n",
    "                if 'num_posts' not in sent.keys():\n",
    "                    sent['num_posts'] = 0\n",
    "                candidate[s] = sent\n",
    "            \n",
    "            if len(avgs.keys()) > 0:\n",
    "                avg_sentiment = max(avgs.keys())\n",
    "                sentiment = avgs[avg_sentiment]\n",
    "                candidate['avg_sentiment'] = avg_sentiment\n",
    "                candidate['sentiment'] = sentiment\n",
    "                state_sent[c] = candidate\n",
    "        stats[k] = state_sent\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a0f7248c-d9b7-4c2a-88b5-9da80848f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_cand_stats = get_sentiment_stats(after_cand_sentiments)\n",
    "before_cand_stats = get_sentiment_stats(before_cand_sentiments)\n",
    "after_cand_comment_stats = get_sentiment_stats(after_cand_comment_sentiments)\n",
    "before_cand_comment_stats = get_sentiment_stats(before_cand_comment_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9db17ccc-e8f7-4265-9298-a880ad4c9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('data/candidate_sentiments/after_cand_post_sentiments.json', after_cand_stats)\n",
    "save_data('data/candidate_sentiments/before_cand_post_sentiments.json', before_cand_stats)\n",
    "save_data('data/candidate_sentiments/after_cand_comment_sentiments.json', after_cand_comment_stats)\n",
    "save_data('data/candidate_sentiments/before_cand_comment_sentiments.json', before_cand_comment_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9528fde-e116-43bc-b99a-91be5f02734c",
   "metadata": {},
   "source": [
    "## Political Direction Analysis\n",
    "Get political direction for each topic. Might look like:\n",
    "```\n",
    "'texas': {\n",
    "    'election': {\n",
    "        'left': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       },\n",
    "       'center': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       }\n",
    "       'right': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       },\n",
    "       'direction': 'left/center/right',\n",
    "       'avg_score': 0.0\n",
    "    },\n",
    "    'abortion': {\n",
    "        'left': {\n",
    "            ...\n",
    "       },\n",
    "       'center': {\n",
    "            ...\n",
    "       }\n",
    "       'right': {\n",
    "            ...\n",
    "       },\n",
    "       'direction': 'left/center/right',\n",
    "       'avg_score': 0.0\n",
    "    ...\n",
    "},\n",
    "'california': {\n",
    "    'election': {\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0fbf0120-7425-42ed-aede-3cb4306a8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will group the keywords we came up with into broader topics\n",
    "topics_dict = {\n",
    "    'election': ['election', 'president', 'vote', 'voting'],\n",
    "    'abortion': ['abortion', 'reproductive rights'],\n",
    "    'immigration': ['immigration', 'immigrant', 'refugee'],\n",
    "    'economy': ['economy', 'inflation', 'tax'],\n",
    "    'war': ['war', 'ukraine', 'israel', 'palestine'],\n",
    "    'democracy': ['democracy', 'freedom', 'stop the steal'],\n",
    "    'climate': ['climate', 'climate change', 'global warming'],\n",
    "    'healthcare': ['healthcare', 'health', 'medical']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6b5a0717-6544-4ee5-8afc-aed50203c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group filtered political posts \n",
    "def group_political_posts(posts):\n",
    "    grouped_posts = dict()\n",
    "        \n",
    "    for k in posts.keys():\n",
    "        grouped_posts[k] = dict()\n",
    "        state_posts = posts[k]\n",
    "\n",
    "        for topic in topics_dict.keys():\n",
    "            post_set = set()\n",
    "            \n",
    "            for post in state_posts:\n",
    "                for word in topics_dict[topic]:\n",
    "                    if word in post[\"selftext\"].lower() or word in post[\"title\"].lower():\n",
    "                        post_set.add(post[\"selftext\"] + \" \" + post[\"title\"])\n",
    "                grouped_posts[k][topic] = list(post_set)\n",
    "    return grouped_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "29faff73-a95a-49c9-b85d-4fd726533154",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_posts_after = group_political_posts(after_political_posts)\n",
    "grouped_posts_before = group_political_posts(before_political_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f5396ce8-94ca-4047-a81d-88835f9c5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group filtered political comments \n",
    "def group_political_comments(comments):\n",
    "    grouped_comments = dict()\n",
    "        \n",
    "    for k in comments.keys():\n",
    "        grouped_comments[k] = dict()\n",
    "        state_comments = comments[k]\n",
    "\n",
    "        for topic in topics_dict.keys():\n",
    "            comment_set = set()\n",
    "                \n",
    "            for comment in state_comments:\n",
    "                for word in topics_dict[topic]:\n",
    "                    if word in comment[\"body\"].lower():\n",
    "                        comment_set.add(comment[\"body\"])\n",
    "                grouped_comments[k][topic] = list(comment_set)\n",
    "    return grouped_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "287d1044-db40-497d-9cc0-fabca4f90b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_comments_after = group_political_comments(after_political_comments)\n",
    "grouped_comments_before = group_political_comments(before_political_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d5f2a1c8-b4e5-4a20-b775-58e90cbb7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out size of grouped posts per state - for testing\n",
    "def count_grouped_posts(grouped_posts):\n",
    "    group_posts_size = dict()\n",
    "    for k in grouped_posts.keys():\n",
    "        group_posts_size[k] = dict()\n",
    "        \n",
    "        for t in grouped_posts[k].keys(): \n",
    "            group_posts_size[k][t] = len(grouped_posts[k][t])\n",
    "    return group_posts_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b77bf00e-c840-4c08-82a4-50e14a40ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_grouped_posts(grouped_posts_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ced1176e-4538-4414-ab60-b8ee99ef8f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nevada': {'election': 358,\n",
       "  'abortion': 5,\n",
       "  'immigration': 13,\n",
       "  'economy': 30,\n",
       "  'war': 44,\n",
       "  'democracy': 16,\n",
       "  'climate': 6,\n",
       "  'healthcare': 21},\n",
       " 'wyoming': {'election': 43,\n",
       "  'abortion': 90,\n",
       "  'immigration': 5,\n",
       "  'economy': 9,\n",
       "  'war': 11,\n",
       "  'democracy': 6,\n",
       "  'climate': 10,\n",
       "  'healthcare': 62}}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_grouped_posts(grouped_comments_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1061ea36-1680-4609-96ff-54aa17b941bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_dir_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "pol_dir_model = AutoModelForSequenceClassification.from_pretrained(\"bucketresearch/politicalBiasBERT\")\n",
    "dir_map = {\n",
    "    0: 'left',\n",
    "    1: 'center',\n",
    "    2: 'right'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "649770d1-b28f-4931-984a-d61d4a5e98a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24744857847690582, 0.17667832970619202, 0.5758731365203857]\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "text = \"I don't believe in climate change\"\n",
    "inputs = pol_dir_tokenizer(text, return_tensors=\"pt\")\n",
    "labels = torch.tensor([0])\n",
    "outputs = pol_dir_model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "print(logits.softmax(dim=-1)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6f884e9d-7c5f-4e03-b5e9-425c2bd5e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute highest direction score and label for given text\n",
    "def get_direction_label_score(text):\n",
    "    inputs = pol_dir_tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    labels = torch.tensor([0])\n",
    "    outputs = pol_dir_model(**inputs, labels=labels)\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    # ranking = list of labels in decreasing order of direction score\n",
    "    scores = logits.softmax(dim=-1)[0].tolist()\n",
    "    ranking = np.argsort(logits.softmax(dim=-1)[0].tolist())[::-1]\n",
    "\n",
    "    # return largest score and the associated sentiment\n",
    "    l = dir_map[ranking[0]]\n",
    "    s = scores[ranking[0]]\n",
    "    return l, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b3085052-3fb7-4baa-ad21-51d65c6760ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('left', 0.4164985120296478)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST\n",
    "get_direction_label_score(\"trans lives matter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1d12dbf5-4bff-4045-b484-4a926419848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction_scores_grouped_posts(grouped_posts):\n",
    "    all_directions = dict()\n",
    "    \n",
    "    for k in grouped_posts.keys():\n",
    "        all_directions[k] = dict()\n",
    "        \n",
    "        for topic in grouped_posts[k].keys():\n",
    "            all_directions[k][topic] = dict()\n",
    "            topic_data = {\n",
    "                'left': {},\n",
    "                'center': {},\n",
    "                'right': {},\n",
    "            }\n",
    "\n",
    "            for post in grouped_posts[k][topic]:\n",
    "                direction, score = get_direction_label_score(post)\n",
    "                record_sentiment(topic_data[direction], score)\n",
    "                \n",
    "                all_directions[k][topic] = topic_data\n",
    "    return all_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "13894199-4a4a-461c-9ed9-c77f81876197",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_cand_post_dir = get_direction_scores_grouped_posts(grouped_posts_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8beb5ed6-1618-4ef6-bd53-77b96b737386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before_cand_post_dir = get_direction_scores_grouped_posts(grouped_posts_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6b2d4fe5-e9c4-4d4f-9000-57318f904b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_cand_comment_dir = get_direction_scores_grouped_posts(grouped_comments_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "308fb795-0fd6-4afd-adf3-6e3f81efdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before_cand_comment_dir = get_direction_scores_grouped_posts(grouped_comments_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8c5b8cff-f9b5-4a0d-bff2-5a8835e4f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('after_cand_post_directions.json', after_cand_post_dir)\n",
    "save_data('after_cand_comment_directions.json', after_cand_comment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bf3c7e5a-1854-4dd9-b226-e351f33b9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average, min, max, and dominant sentiment for each state\n",
    "def get_direction_stats(all_directions):\n",
    "    stats = all_directions.copy()\n",
    "    \n",
    "    for k in stats.keys():\n",
    "        state_dir = stats[k].copy()\n",
    "        \n",
    "        for d in state_dir.keys():\n",
    "            avgs = dict()\n",
    "            topics = state_dir[d].copy()\n",
    "\n",
    "            for t in topics.keys():\n",
    "                topic = topics[t].copy()\n",
    "                if 'scores' not in topic.keys():\n",
    "                    topic['scores'] = [0]\n",
    "                    \n",
    "                topic['min'] = min(topic['scores'])\n",
    "                topic['max'] = max(topic['scores'])\n",
    "                topic['average'] = np.mean(topic['scores'])\n",
    "                avgs[topic['average']] = t\n",
    "                \n",
    "                if 'num_posts' not in topic.keys():\n",
    "                    topic['num_posts'] = 0\n",
    "                topics[t] = topic\n",
    "            \n",
    "            if len(avgs.keys()) > 0:\n",
    "                avg_score = max(avgs.keys())\n",
    "                direction = avgs[avg_score]\n",
    "                topics['avg_score'] = avg_score\n",
    "                topics['direction'] = direction\n",
    "                state_dir[d] = topics\n",
    "        stats[k] = state_dir\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9c09779a-b536-4be0-add8-500aea8d88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_topic_post_stats = get_direction_stats(after_cand_post_dir)\n",
    "after_topic_comment_stats = get_direction_stats(after_cand_comment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "48fa8054-fd2a-48bf-a305-55080ede2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('data/topic_pol_directions/after_topic_post_stats.json', after_topic_post_stats)\n",
    "save_data('data/topic_pol_directions/after_topic_comment_stats.json', after_topic_comment_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83c5fe-c466-4c19-b0da-6f8517b01634",
   "metadata": {},
   "source": [
    "# ===== OLD STUFF ===== DO NOT RUN ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9749591-6ad0-4a38-8771-ba75754b52e7",
   "metadata": {},
   "source": [
    "## Option 1: take sentiment of entire post, give an average rating\n",
    "\n",
    "Get the aggregated sentiment score for each post. Might want something like:\n",
    "```all_sentiments = {\n",
    "    'texas': {\n",
    "        'positive': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       },\n",
    "       'neutral': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       }\n",
    "       'negative': {\n",
    "           'scores': [0, 0, ...],\n",
    "           'average': 0.0,\n",
    "           'max': 0.0,\n",
    "           'min': 0.0,\n",
    "           'num_posts': 0\n",
    "       },\n",
    "       'sentiment': 'positive',\n",
    "       'avg_sentiment': 0.0\n",
    "    },\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01b8f7ae-470c-4c8c-b6c8-af7d62a10493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for creating the json above\n",
    "def record_sentiment(sent_data, score):\n",
    "    if \"scores\" in sent_data.keys():\n",
    "        sent_data[\"scores\"].append(score)\n",
    "    else:\n",
    "        sent_data[\"scores\"] = [score]\n",
    "\n",
    "    if \"num_posts\" in sent_data.keys():\n",
    "        sent_data[\"num_posts\"] += 1\n",
    "    else:\n",
    "        sent_data[\"num_posts\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "174ecf5a-cf72-47e9-980c-c1f3b3d9ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each state and compute highest weight sentiment for each post\n",
    "def get_sentiment_scores_posts(political_posts):\n",
    "    all_sentiment = dict()\n",
    "    \n",
    "    for k in political_posts.keys():\n",
    "        state_data = {\n",
    "            'positive': {},\n",
    "            'neutral': {},\n",
    "            'negative': {},\n",
    "        }\n",
    "        \n",
    "        for post in political_posts[k]:\n",
    "            # as we are not working with semantic meaning, we will combine the text and titles of the posts\n",
    "            content = post['title'] + \" \" + post['selftext']\n",
    "            label, score = get_sentiment_label_score(content)\n",
    "            record_sentiment(state_data[label], score)\n",
    "            \n",
    "            all_sentiment[k] = state_data\n",
    "    return all_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40510529-b0ab-4118-87f8-6df57ffc9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the comments version of the above\n",
    "def get_sentiment_scores_comments(political_comments):\n",
    "    all_sentiment = dict()\n",
    "    \n",
    "    for k in political_comments.keys():\n",
    "        state_data = {\n",
    "            'positive': {},\n",
    "            'neutral': {},\n",
    "            'negative': {},\n",
    "        }\n",
    "        \n",
    "        for post in political_comments[k]:\n",
    "            content = post['body']\n",
    "            label, score = get_sentiment_label_score(content)\n",
    "            record_sentiment(state_data[label], score)\n",
    "            \n",
    "            all_sentiment[k] = state_data\n",
    "    return all_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6eb23aec-2954-4bb7-ae38-fd93036dea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after_post_sentiments = get_sentiment_scores_posts(after_political_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28f36fca-aba2-4ec0-bb1e-33f39ad07e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before_post_sentiments = get_sentiment_scores_posts(before_political_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "000139b2-7746-4b8b-8427-822bb589ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after_comment_sentiments = get_sentiment_scores_comments(after_political_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f97a02bc-95e8-4506-9e01-0730f5f944df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before_comment_sentiments = get_sentiment_scores_comments(before_political_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e15ad-3803-4cd8-8608-9bdc0790cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_post_sentiments = extract_data('reddit-sentiment-analysis/sentiments/all_posts/after_post_sentiments.json')\n",
    "# before_post_sentiments = extract_data('reddit-sentiment-analysis/sentiments/all_posts/before_post_sentiments.json')\n",
    "# after_comment_sentiments = extract_data('reddit-sentiment-analysis/sentiments/all_posts/after_comments_sentiments.json')\n",
    "# before_comment_sentiments = extract_data('reddit-sentiment-analysis/sentiments/all_posts/before_comments_sentiments.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85661625-1f1a-4c86-848e-2b36d9ed0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average, min, max, and dominant sentiment for each state\n",
    "def get_sentiment_stats(all_sentiment):\n",
    "    stats = dict()\n",
    "    for k in all_sentiment.keys():\n",
    "        avgs = dict()\n",
    "        stats[k] = dict()\n",
    "        state_sent = all_sentiment[k]\n",
    "        \n",
    "        for s in state_sent.keys():\n",
    "            sent = state_sent[s]\n",
    "            if 'scores' not in sent.keys():\n",
    "                sent['scores'] = [0]\n",
    "                \n",
    "            sent['min'] = min(sent['scores'])\n",
    "            sent['max'] = max(sent['scores'])\n",
    "            sent['average'] = np.mean(sent['scores'])\n",
    "            avgs[sent['average']] = s\n",
    "            \n",
    "            if 'num_posts' not in sent.keys():\n",
    "                sent['num_posts'] = 0\n",
    "    \n",
    "        avg_sentiment = max(avgs.keys())\n",
    "        sentiment = avgs[avg_sentiment]\n",
    "        stats[k]['avg_sentiment'] = avg_sentiment\n",
    "        stats[k]['sentiment'] = sentiment\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48a31f26-a4ee-4979-ae5c-3478c92ff56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_post_sentiments_stats = get_sentiment_stats(after_post_sentiments)\n",
    "before_post_sentiments_stats = get_sentiment_stats(before_post_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4dd6132e-4d5d-4d6a-b7bd-43ed597f27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_comment_sentiments_stats = get_sentiment_stats(after_comment_sentiments)\n",
    "before_comment_sentiments_stats = get_sentiment_stats(before_comment_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2d4b987-adc0-4bf3-a9aa-21fede2de744",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('after_post_sentiments.json', after_post_sentiments)\n",
    "save_data('before_post_sentiments.json', before_post_sentiments)\n",
    "save_data('after_comment_sentiments.json', after_comment_sentiments)\n",
    "save_data('before_comment_sentiments.json', before_comment_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98014391-d80e-45d3-a7aa-aded9d1cdfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('after_post_stats.json', after_post_sentiments_stats)\n",
    "save_data('before_post_stats.json', before_post_sentiments_stats)\n",
    "save_data('after_comment_stats.json', after_comment_sentiments_stats)\n",
    "save_data('before_comment_stats.json', before_comment_sentiments_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814244a-6a14-44aa-a25b-5e178a740d9a",
   "metadata": {},
   "source": [
    "## Option 2: Get sentiment for each topic per state, print the same statistics as above\n",
    "Further subdivide the political posts by their main topics. Data will look something like this:\n",
    "```all_sentiments = {\n",
    "    'texas': {\n",
    "        'election': {\n",
    "            'positive': {\n",
    "                   'scores': [0, 0, ...],\n",
    "                   'average': 0.0,\n",
    "                   'max': 0.0,\n",
    "                   'min': 0.0,\n",
    "                   'num_posts': 0\n",
    "               },\n",
    "               'neutral': {\n",
    "                   'scores': [0, 0, ...],\n",
    "                   'average': 0.0,\n",
    "                   'max': 0.0,\n",
    "                   'min': 0.0,\n",
    "                   'num_posts': 0\n",
    "               }\n",
    "               'negative': {\n",
    "                   'scores': [0, 0, ...],\n",
    "                   'average': 0.0,\n",
    "                   'max': 0.0,\n",
    "                   'min': 0.0,\n",
    "                   'num_posts': 0\n",
    "               },\n",
    "               'sentiment': 'positive',\n",
    "               'avg_sentiment': 0.0\n",
    "            },\n",
    "        },\n",
    "        'republican': {\n",
    "            'positive': {\n",
    "                ...\n",
    "               },\n",
    "               'neutral': {\n",
    "                ...\n",
    "               }\n",
    "               'negative': {\n",
    "                ...\n",
    "               },\n",
    "               'sentiment': 'positive',\n",
    "               'avg_sentiment': 0.0\n",
    "            },\n",
    "        },\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "03c59abb-42bb-47c2-b2e1-cdd7861286a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group filtered political posts \n",
    "def group_political_posts(posts):\n",
    "    grouped_posts = dict()\n",
    "        \n",
    "    for k in posts.keys():\n",
    "        grouped_posts[k] = dict()\n",
    "        state_posts = posts[k]\n",
    "\n",
    "        for post in state_posts:\n",
    "            for topic in topics_dict.keys():\n",
    "                post_set = set()\n",
    "                \n",
    "                for word in topics_dict[topic]:\n",
    "                    if word in post[\"selftext\"].lower() or word in post[\"title\"].lower():\n",
    "                        post_set.add(post[\"selftext\"] + \" \" + post[\"title\"])\n",
    "                grouped_posts[k][topic] = list(post_set)\n",
    "    return grouped_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "282bfd75-2b0b-4711-8198-9ae706a8895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group filtered political comments \n",
    "def group_political_comments(comments):\n",
    "    grouped_comments = dict()\n",
    "        \n",
    "    for k in comments.keys():\n",
    "        grouped_comments[k] = dict()\n",
    "        state_comments = comments[k]\n",
    "\n",
    "        for comment in state_comments:\n",
    "            for topic in topics_dict.keys():\n",
    "                comment_set = set()\n",
    "                \n",
    "                for word in topics_dict[topic]:\n",
    "                    if word in comment[\"body\"].lower():\n",
    "                        comment_set.add(comment[\"body\"])\n",
    "                grouped_comments[k][topic] = list(comment_set)\n",
    "    return grouped_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e69e6a0e-4ec5-475a-91e4-8ad4fca57972",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_posts_after = group_political_posts(after_political_posts)\n",
    "grouped_posts_before = group_political_posts(before_political_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4513714e-d6b5-43f5-8e16-c6e911d6b721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'election': [],\n",
       " 'republican': [' ‘Bullish on Michigan’: As polls close, Republicans are optimistic on Trump’s chances'],\n",
       " 'democrat': [],\n",
       " 'abortion': [],\n",
       " 'immigration': [],\n",
       " 'economy': [],\n",
       " 'war': [],\n",
       " 'democracy': [],\n",
       " 'climate': [],\n",
       " 'healthcare': []}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_posts_after['michigan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2a051af-fbb6-44c7-bde0-5770c5cddb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_comments_after = group_political_comments(after_political_comments)\n",
    "grouped_comments_before = group_political_comments(before_political_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2c31e0c2-4523-498b-b3bf-65a83328b882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'election': ['So it sounds like me, you, and David Duke would be pretty close on Israel policy; if you nut jobs wanna blow each other up, be my guest, but not with my guns, not with my taxes, and sure as hell not with American lives. Would you agree with this?\\n\\nAnd yes, Cheney did shoot a guy in the face. I think it\\'s maybe one of the nicer things Dick has ever done in his life. He also lied to the entire world about WMD\\'s to push the United States into an unquestionably immoral war, killing thousands of Americans and hundreds of thousands of Iraqis. David Duke, on the other hand, is a loud racist idiot that hasn\\'t done anything meaningful in his entire life. I think one of these people is a clear threat to freedom and democracy in this country. \\n\\nAlso, I find it a bit ironic that in your mind, \"whoever the Nazis are voting for\" has been determined to be Jill Stein, the farthest left and only Jewish candidate.'],\n",
       " 'republican': [],\n",
       " 'democrat': [],\n",
       " 'abortion': [],\n",
       " 'immigration': [],\n",
       " 'economy': ['So it sounds like me, you, and David Duke would be pretty close on Israel policy; if you nut jobs wanna blow each other up, be my guest, but not with my guns, not with my taxes, and sure as hell not with American lives. Would you agree with this?\\n\\nAnd yes, Cheney did shoot a guy in the face. I think it\\'s maybe one of the nicer things Dick has ever done in his life. He also lied to the entire world about WMD\\'s to push the United States into an unquestionably immoral war, killing thousands of Americans and hundreds of thousands of Iraqis. David Duke, on the other hand, is a loud racist idiot that hasn\\'t done anything meaningful in his entire life. I think one of these people is a clear threat to freedom and democracy in this country. \\n\\nAlso, I find it a bit ironic that in your mind, \"whoever the Nazis are voting for\" has been determined to be Jill Stein, the farthest left and only Jewish candidate.'],\n",
       " 'war': ['So it sounds like me, you, and David Duke would be pretty close on Israel policy; if you nut jobs wanna blow each other up, be my guest, but not with my guns, not with my taxes, and sure as hell not with American lives. Would you agree with this?\\n\\nAnd yes, Cheney did shoot a guy in the face. I think it\\'s maybe one of the nicer things Dick has ever done in his life. He also lied to the entire world about WMD\\'s to push the United States into an unquestionably immoral war, killing thousands of Americans and hundreds of thousands of Iraqis. David Duke, on the other hand, is a loud racist idiot that hasn\\'t done anything meaningful in his entire life. I think one of these people is a clear threat to freedom and democracy in this country. \\n\\nAlso, I find it a bit ironic that in your mind, \"whoever the Nazis are voting for\" has been determined to be Jill Stein, the farthest left and only Jewish candidate.'],\n",
       " 'democracy': ['So it sounds like me, you, and David Duke would be pretty close on Israel policy; if you nut jobs wanna blow each other up, be my guest, but not with my guns, not with my taxes, and sure as hell not with American lives. Would you agree with this?\\n\\nAnd yes, Cheney did shoot a guy in the face. I think it\\'s maybe one of the nicer things Dick has ever done in his life. He also lied to the entire world about WMD\\'s to push the United States into an unquestionably immoral war, killing thousands of Americans and hundreds of thousands of Iraqis. David Duke, on the other hand, is a loud racist idiot that hasn\\'t done anything meaningful in his entire life. I think one of these people is a clear threat to freedom and democracy in this country. \\n\\nAlso, I find it a bit ironic that in your mind, \"whoever the Nazis are voting for\" has been determined to be Jill Stein, the farthest left and only Jewish candidate.'],\n",
       " 'climate': [],\n",
       " 'healthcare': []}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_comments_after['nevada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eea1c59a-aad5-4afe-8d60-ce1697a1b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out size of grouped posts per state - for testing\n",
    "def count_grouped_posts(grouped_posts):\n",
    "    group_posts_size = dict()\n",
    "    for k in grouped_posts.keys():\n",
    "        group_posts_size[k] = dict()\n",
    "        \n",
    "        for t in grouped_posts[k].keys(): \n",
    "            group_posts_size[k][t] = len(grouped_posts[k][t])\n",
    "    return group_posts_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5277da98-39ce-4cd8-9bf4-e675836be5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_grouped_posts(grouped_posts_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65f21e88-a97c-4961-91fe-8c1ce7c3b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_grouped_posts(grouped_posts_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "be3eac7d-01a1-4503-935d-8a07f699ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_grouped_posts(grouped_comments_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8de60371-041c-41bd-8b3b-b791c2d51dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_grouped_posts(grouped_comments_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eaddb5a2-537f-4397-8770-3f5e15a203f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores_grouped_posts(grouped_posts):\n",
    "    all_sentiment = dict()\n",
    "    \n",
    "    for k in grouped_posts.keys():\n",
    "        all_sentiment[k] = dict()\n",
    "        \n",
    "        for topic in grouped_posts[k].keys():\n",
    "            all_sentiment[k][topic] = dict()\n",
    "            topic_data = {\n",
    "                'positive': {},\n",
    "                'neutral': {},\n",
    "                'negative': {},\n",
    "            }\n",
    "\n",
    "            for post in grouped_posts[k][topic]:\n",
    "                label, score = get_sentiment_label_score(post)\n",
    "                record_sentiment(topic_data[label], score)\n",
    "                \n",
    "                all_sentiment[k][topic] = topic_data\n",
    "    return all_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "38b883ee-a4d9-4034-ac79-af5ffd297925",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sentiment_per_topic_after = get_sentiment_scores_grouped_posts(grouped_posts_after)\n",
    "post_sentiment_per_topic_before = get_sentiment_scores_grouped_posts(grouped_posts_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ccb9f57f-17f6-426d-ae63-8cae73f6e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_sentiment_per_topic_after = get_sentiment_scores_grouped_posts(grouped_comments_after)\n",
    "comment_sentiment_per_topic_before = get_sentiment_scores_grouped_posts(grouped_comments_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6b01229f-f2e8-4bbd-87f7-91dd74c75175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average, min, max, and dominant sentiment for each topic per state\n",
    "def get_sentiment_stats(all_sentiment):\n",
    "    stats = dict()\n",
    "    for k in all_sentiment.keys():\n",
    "        stats[k] = dict()\n",
    "        \n",
    "        for topic in all_sentiment[k].keys():\n",
    "            stats[k][topic] = dict()\n",
    "            topic_sent = all_sentiment[k][topic]\n",
    "            avgs = dict()\n",
    "        \n",
    "            for s in topic_sent.keys():\n",
    "                sent = topic_sent[s]\n",
    "                if 'scores' not in sent.keys():\n",
    "                    sent['scores'] = [0]\n",
    "                    \n",
    "                sent['min'] = min(sent['scores'])\n",
    "                sent['max'] = max(sent['scores'])\n",
    "                sent['average'] = np.mean(sent['scores'])\n",
    "                avgs[sent['average']] = s\n",
    "                \n",
    "                if 'num_posts' not in sent.keys():\n",
    "                    sent['num_posts'] = 0\n",
    "        \n",
    "            if len(avgs.keys()) > 0:\n",
    "                avg_sentiment = max(avgs.keys())\n",
    "                sentiment = avgs[avg_sentiment]\n",
    "                stats[k][topic]['avg_sentiment'] = avg_sentiment\n",
    "                stats[k][topic]['sentiment'] = sentiment\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4f46b8f8-636c-44ce-9022-dd74816b9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_stats_per_topic_after = get_sentiment_stats(post_sentiment_per_topic_after)\n",
    "post_stats_per_topic_before = get_sentiment_stats(post_sentiment_per_topic_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7141f904-34c0-4fb0-928c-4fcbac3cece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_stats_per_topic_after = get_sentiment_stats(comment_sentiment_per_topic_after)\n",
    "comment_stats_per_topic_before = get_sentiment_stats(comment_sentiment_per_topic_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3f391d76-3686-4999-85f4-af3ffc062fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('after_post_sentiments_per_topic.json', post_sentiment_per_topic_after)\n",
    "save_data('before_post_sentiments_per_topic.json', post_sentiment_per_topic_before)\n",
    "save_data('after_comment_sentiments_per_topic.json', comment_sentiment_per_topic_after)\n",
    "save_data('before_comment_sentiments_per_topic.json', comment_sentiment_per_topic_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e705f1cd-9c88-4aca-b89b-b17ab3185c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('after_post_stats_per_topic.json', post_stats_per_topic_after)\n",
    "save_data('before_post_stats_per_topic.json', post_stats_per_topic_before)\n",
    "save_data('after_comment_stats_per_topic.json', comment_stats_per_topic_after)\n",
    "save_data('before_comment_stats_per_topic.json', comment_stats_per_topic_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95892e5-8ec8-41e0-983b-8e5eed4b27a5",
   "metadata": {},
   "source": [
    "## Testing Topic Modeling for Political Data Extraction\n",
    "Probably don't run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94c0da96-ae4c-4f9e-ae3f-cd741e896545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic.load(\"MaartenGr/BERTopic_Wikipedia\")\n",
    "\n",
    "model_topics = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c6ca2b2-d388-476e-99fe-25ec5b81a56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>633881</td>\n",
       "      <td>-1_cast_films_film_movie</td>\n",
       "      <td>[cast, films, film, movie, 2020, comedy, relea...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>18441</td>\n",
       "      <td>0_goalscorer_scored_goals_goal</td>\n",
       "      <td>[goalscorer, scored, goals, goal, goalkeeper, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8518</td>\n",
       "      <td>1_khan_actor_raj_shah</td>\n",
       "      <td>[khan, actor, raj, shah, crore, hai, actress, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7521</td>\n",
       "      <td>2_married_divorced_couple_remarried</td>\n",
       "      <td>[married, divorced, couple, remarried, engaged...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6765</td>\n",
       "      <td>3_cast_actress_starred_actor</td>\n",
       "      <td>[cast, actress, starred, actor, actors, starri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2371</td>\n",
       "      <td>30</td>\n",
       "      <td>2371_paintings_painting_paint_art</td>\n",
       "      <td>[paintings, painting, paint, art, artist, gall...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2372</td>\n",
       "      <td>30</td>\n",
       "      <td>2372_tulips_tulip_economists_economic</td>\n",
       "      <td>[tulips, tulip, economists, economic, bulbs, 1...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2373</td>\n",
       "      <td>30</td>\n",
       "      <td>2373_squads_squad_roster_players</td>\n",
       "      <td>[squads, squad, roster, players, teams, tourna...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2374</td>\n",
       "      <td>30</td>\n",
       "      <td>2374_entrances_subterranean_tunnel_stairs</td>\n",
       "      <td>[entrances, subterranean, tunnel, stairs, pyra...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>2375</td>\n",
       "      <td>30</td>\n",
       "      <td>2375_transhumanism_transhumanists_transhumanis...</td>\n",
       "      <td>[transhumanism, transhumanists, transhumanist,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2377 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic   Count                                               Name  \\\n",
       "0        -1  633881                           -1_cast_films_film_movie   \n",
       "1         0   18441                     0_goalscorer_scored_goals_goal   \n",
       "2         1    8518                              1_khan_actor_raj_shah   \n",
       "3         2    7521                2_married_divorced_couple_remarried   \n",
       "4         3    6765                       3_cast_actress_starred_actor   \n",
       "...     ...     ...                                                ...   \n",
       "2372   2371      30                  2371_paintings_painting_paint_art   \n",
       "2373   2372      30              2372_tulips_tulip_economists_economic   \n",
       "2374   2373      30                   2373_squads_squad_roster_players   \n",
       "2375   2374      30          2374_entrances_subterranean_tunnel_stairs   \n",
       "2376   2375      30  2375_transhumanism_transhumanists_transhumanis...   \n",
       "\n",
       "                                         Representation  Representative_Docs  \n",
       "0     [cast, films, film, movie, 2020, comedy, relea...                  NaN  \n",
       "1     [goalscorer, scored, goals, goal, goalkeeper, ...                  NaN  \n",
       "2     [khan, actor, raj, shah, crore, hai, actress, ...                  NaN  \n",
       "3     [married, divorced, couple, remarried, engaged...                  NaN  \n",
       "4     [cast, actress, starred, actor, actors, starri...                  NaN  \n",
       "...                                                 ...                  ...  \n",
       "2372  [paintings, painting, paint, art, artist, gall...                  NaN  \n",
       "2373  [tulips, tulip, economists, economic, bulbs, 1...                  NaN  \n",
       "2374  [squads, squad, roster, players, teams, tourna...                  NaN  \n",
       "2375  [entrances, subterranean, tunnel, stairs, pyra...                  NaN  \n",
       "2376  [transhumanism, transhumanists, transhumanist,...                  NaN  \n",
       "\n",
       "[2377 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f6c096c9-b78b-4c30-b90d-d5c6ebf33754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the numbers of topics that contain our list of keywords\n",
    "political_topic_nums = []\n",
    "for w in keywords:\n",
    "    for i in range(len(model_topics[\"Representation\"])):\n",
    "        if w in model_topics[\"Representation\"][i]:\n",
    "            political_topic_nums.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ebfeefa1-e7dc-468c-81a7-cd21d5ea8c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 56,\n",
       " 74,\n",
       " 75,\n",
       " 125,\n",
       " 243,\n",
       " 272,\n",
       " 320,\n",
       " 359,\n",
       " 391,\n",
       " 556,\n",
       " 608,\n",
       " 718,\n",
       " 876,\n",
       " 1010,\n",
       " 1033,\n",
       " 1068,\n",
       " 1147,\n",
       " 1217,\n",
       " 1259,\n",
       " 1296,\n",
       " 1664,\n",
       " 1879,\n",
       " 1963,\n",
       " 1996,\n",
       " 1997,\n",
       " 2232,\n",
       " 2299,\n",
       " 363,\n",
       " 608,\n",
       " 782,\n",
       " 939,\n",
       " 1147,\n",
       " 1458,\n",
       " 1594,\n",
       " 1681,\n",
       " 1729,\n",
       " 1856,\n",
       " 2123,\n",
       " 2140,\n",
       " 10,\n",
       " 56,\n",
       " 90,\n",
       " 202,\n",
       " 272,\n",
       " 363,\n",
       " 372,\n",
       " 449,\n",
       " 782,\n",
       " 939,\n",
       " 1033,\n",
       " 1147,\n",
       " 1458,\n",
       " 1594,\n",
       " 1681,\n",
       " 1729,\n",
       " 2123,\n",
       " 2140,\n",
       " 74,\n",
       " 359,\n",
       " 1217,\n",
       " 10,\n",
       " 1217,\n",
       " 1879,\n",
       " 286,\n",
       " 503,\n",
       " 1010,\n",
       " 1681,\n",
       " 1856,\n",
       " 235,\n",
       " 523,\n",
       " 745,\n",
       " 1639,\n",
       " 290,\n",
       " 601,\n",
       " 1259,\n",
       " 1318,\n",
       " 1962,\n",
       " 535,\n",
       " 841,\n",
       " 966,\n",
       " 1404,\n",
       " 1998,\n",
       " 2352,\n",
       " 2366,\n",
       " 23,\n",
       " 878,\n",
       " 1179,\n",
       " 543,\n",
       " 590,\n",
       " 714,\n",
       " 957,\n",
       " 261,\n",
       " 455,\n",
       " 674,\n",
       " 1954,\n",
       " 2028,\n",
       " 2040,\n",
       " 2344]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_topic_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aa483c04-b1ac-4d55-a0f3-757863ca7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document needs to be a list to pass into topic modelling model\n",
    "# Create a new json of { state1: [ \"title1, text1\", \"title2, text2\", ... ], state2: [\"title1, text1\", ...]\n",
    "state_to_post = dict()\n",
    "\n",
    "for k in political_posts.keys():\n",
    "    posts = []\n",
    "    for post in political_posts[k]:\n",
    "        post_str = post['title'] + \" \" + post['selftext']\n",
    "        posts.append(post_str)\n",
    "    state_to_post[k] = posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3000e581-89c5-4b89-a6cc-f77d37d885b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7253da2d6e4040eab20d1165797bc2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:11,380 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53786ae3a0e045d2926946548892474a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:14,057 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9794d79769649bebb5cffd70a855fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:15,479 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c16f5ff38c40289011e5782d99bdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:15,700 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0a370a0fb24e3992946b98872d188b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:15,789 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894cbeaa44974c4a97e502dd631429b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:19,162 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61714faba11c40c1b2f483fb59f5fe6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:20,597 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26aaa5c3927416fbc1555412649a3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:21,001 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6251c2361454451951554d100c8fb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:21,072 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d47393e53c2470c82570760488fa50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:21,330 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d1d8024a634f978e338aa5b8080e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:22,135 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f3eca4f0344bd1b98da297a02ff06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:22,772 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a523e4b8600446ae8fafd825621cebef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:23,085 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c36c574b824484a85ed203bdf0a34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:23,274 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5afa5c1ba294ca89eaf3e19e260ead0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:26,794 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da06b1b86dc94d54a804c9d61b869153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:30,724 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59f9b25bed340359afc64db4bb587c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:30,820 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3b488b63404fd499e921fb64f3d251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:32,397 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e397636229fa4a9e9a84f5855a4cc4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:33,001 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70461f6e26854266a140ce8c1c473167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:33,261 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4def7f4efbbf4fb99db6c986e29dde1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:34,864 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f105edc3ff904bd79c8c9402c76524a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:35,114 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692395ffeb7b4a5699f31c83ec780348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:35,296 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed6ee87e9c3449baf55663b773dff64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:35,643 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfff3dfd6fb4cbe84173a599edc5e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:40,180 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cdaeb8e6c247d89025a5a43f7dfa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:40,498 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b610ed4ddc7c45fe9f67a5e3d99adce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:41,101 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd9239b50ff4c4693f3ba32bb61a9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:41,285 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e90934285943bcb83b72d9d3a69381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:43,414 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277047718d124773beab97ecbffda957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:43,551 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4212333cc68f430f91ef901c1cfb6b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:43,996 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93572a31c6b45aaa5faa104199da77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:44,240 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab80e1a751a2405a9164a1207c225c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:44,580 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd69f2342f4e4ec3883f09f88cce3da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:44,684 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adac27c4d78545d9bf02aa6395a3ffc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:45,394 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e386bb6eac04f0f840b095a02253129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:47,458 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbaefc179c64e06bac8875e56078b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:48,154 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f63ca55ad904c2cb0843450d8a5772b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:49,988 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074cccf33d80417c80cc179197f4bbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:50,334 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f10cc8530f48c69b84b290730b65aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:51,135 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdde3c602fc34785999f891005a83140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:51,796 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b58b47f48ff4e1097b71347eb237544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:52,602 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e8d4f6c30b440fa19c6a6410874879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:53,217 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91582da7df3b4c8b911ad3d087c6cc6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:53,755 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9c217d4d144fe6b89c2cf38ff72ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:57,285 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd87e1336e5540ff9115e64636bc3918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:57,925 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef76bad7c5be4a5199c6f119558eab81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:16:59,724 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    }
   ],
   "source": [
    "state_to_topic = dict()\n",
    "\n",
    "for k in state_to_post.keys():\n",
    "    if len(state_to_post[k]) != 0:\n",
    "        post_topics, post_probs = topic_model.transform(state_to_post[k])\n",
    "        state_to_topic[k] = {\n",
    "            'topics': post_topics,\n",
    "            'probabilities': post_probs\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "37d1577c-6db1-4e66-a2a5-a050d51fb410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'texas': {'topics': array([1889,   65, 1889, 1272, 1889,  234,  234,  234, 1501, 2297,  251]),\n",
       "  'probabilities': array([0.3581864 , 0.3898025 , 0.39754048, 0.33664972, 0.3568266 ,\n",
       "         0.5804087 , 0.5804087 , 0.43511474, 0.39582056, 0.37019277,\n",
       "         0.38129365], dtype=float32)},\n",
       " 'california': {'topics': array([ 308,  308,  308,  673,    9,    9,    9,  542,  542,   20,    9,\n",
       "            9,    9,    9, 1239,  481]),\n",
       "  'probabilities': array([0.37759215, 0.37759215, 0.37759215, 0.6793472 , 0.49634874,\n",
       "         0.49634874, 0.49634874, 0.6666453 , 0.6666453 , 0.44082233,\n",
       "         0.39934936, 0.39934936, 0.39934936, 0.39934936, 0.43308628,\n",
       "         0.59780335], dtype=float32)},\n",
       " 'michigan': {'topics': array([ 260,  260, 1213,  461, 1615,  542]),\n",
       "  'probabilities': array([0.4505028 , 0.4505028 , 0.41623247, 0.45489872, 0.3821689 ,\n",
       "         0.41819587], dtype=float32)},\n",
       " 'minnesota': {'topics': array([313, 436,  22]),\n",
       "  'probabilities': array([0.4170059 , 0.3053832 , 0.31694606], dtype=float32)},\n",
       " 'colorado': {'topics': array([9]),\n",
       "  'probabilities': array([0.42447263], dtype=float32)},\n",
       " 'wisconsin': {'topics': array([726, 726, 726, 770, 770, 770,  81,   9,   9,   9, 285, 285, 285,\n",
       "         615,   9, 171,  66]),\n",
       "  'probabilities': array([0.6723514 , 0.6723514 , 0.6723514 , 0.50814164, 0.50814164,\n",
       "         0.50814164, 0.34986448, 0.43974257, 0.43974257, 0.43974257,\n",
       "         0.50689435, 0.50689435, 0.50689435, 0.32239306, 0.47452837,\n",
       "         0.36719084, 0.28068623], dtype=float32)},\n",
       " 'florida': {'topics': array([1539,  271,  271,  960, 2140]),\n",
       "  'probabilities': array([0.51721954, 0.34675372, 0.34385204, 0.39405838, 0.47158602],\n",
       "        dtype=float32)},\n",
       " 'connecticut': {'topics': array([ 622,  622,  622,  875,  875,  589, 1049]),\n",
       "  'probabilities': array([0.36085767, 0.36085767, 0.36085767, 0.4093138 , 0.4093138 ,\n",
       "         0.5329325 , 0.4205263 ], dtype=float32)},\n",
       " 'oregon': {'topics': array([851]),\n",
       "  'probabilities': array([0.35824457], dtype=float32)},\n",
       " 'ohio': {'topics': array([1370,    9, 1239, 1239]),\n",
       "  'probabilities': array([0.3839497 , 0.34234732, 0.6269493 , 0.6269493 ], dtype=float32)},\n",
       " 'northcarolina': {'topics': array([   9,    9,  589,    9,    9,    9,  542,  542,    9, 1213,  542,\n",
       "          542,  542,    9,    9, 2312]),\n",
       "  'probabilities': array([0.4787423 , 0.3822245 , 0.49975592, 0.43024597, 0.43024597,\n",
       "         0.43024597, 0.39940304, 0.39940304, 0.445988  , 0.40880394,\n",
       "         0.44865096, 0.44865096, 0.44865096, 0.38892388, 0.38892388,\n",
       "         0.4019493 ], dtype=float32)},\n",
       " 'oklahoma': {'topics': array([ 114,  480,  480,    9,  917,    9, 1213, 2097,  830, 2270,  451,\n",
       "          533]),\n",
       "  'probabilities': array([0.51299316, 0.44936872, 0.44936872, 0.37969357, 0.40231243,\n",
       "         0.33127764, 0.33871108, 0.43423814, 0.4064579 , 0.42405835,\n",
       "         0.4102937 , 0.46651793], dtype=float32)},\n",
       " 'maryland': {'topics': array([1357, 1854,    9,  956, 1194]),\n",
       "  'probabilities': array([0.3957715 , 0.47808847, 0.3510213 , 0.34898746, 0.4423466 ],\n",
       "        dtype=float32)},\n",
       " 'arizona': {'topics': array([257, 505, 505]),\n",
       "  'probabilities': array([0.3903245 , 0.40109825, 0.40109825], dtype=float32)},\n",
       " 'virginia': {'topics': array([1480,    9, 1889, 2187, 1889,  238,    9,    9,    9,    9,    9,\n",
       "          875,  875,  965,  965,  965]),\n",
       "  'probabilities': array([0.36191547, 0.49175066, 0.46093667, 0.33902255, 0.37063217,\n",
       "         0.3725991 , 0.538269  , 0.538269  , 0.538269  , 0.538269  ,\n",
       "         0.538269  , 0.5198106 , 0.5198106 , 0.40717077, 0.40717077,\n",
       "         0.40717077], dtype=float32)},\n",
       " 'maine': {'topics': array([   9,    9,  401,  791,  791,  791,  791,  791,  424,  424, 1300,\n",
       "         1213, 1824,  424]),\n",
       "  'probabilities': array([0.3580437 , 0.3580437 , 0.4052534 , 0.47005826, 0.47005826,\n",
       "         0.47005826, 0.47005826, 0.47005826, 0.42602462, 0.42602462,\n",
       "         0.4511706 , 0.48828688, 0.375156  , 0.38565752], dtype=float32)},\n",
       " 'indiana': {'topics': array([   9,    9, 1272,    9]),\n",
       "  'probabilities': array([0.33990473, 0.33990473, 0.44432676, 0.3819639 ], dtype=float32)},\n",
       " 'iowa': {'topics': array([1680, 1680, 1680,    9,    9,  285,  285,  296, 2206, 1272,  589]),\n",
       "  'probabilities': array([0.34948137, 0.34948137, 0.34948137, 0.43962845, 0.43962845,\n",
       "         0.5273669 , 0.5273669 , 0.5331259 , 0.41439122, 0.40128088,\n",
       "         0.58088017], dtype=float32)},\n",
       " 'washington': {'topics': array([   9,  114,  114,  114, 1216, 1216, 1216, 1546,  542,  266, 1822,\n",
       "          495]),\n",
       "  'probabilities': array([0.41339093, 0.3447457 , 0.3447457 , 0.3447457 , 0.49986872,\n",
       "         0.49986872, 0.49986872, 0.3997367 , 0.36949596, 0.65483034,\n",
       "         0.33787835, 0.3516096 ], dtype=float32)},\n",
       " 'newhampshire': {'topics': array([1446, 1446, 1194, 1136]),\n",
       "  'probabilities': array([0.4191046 , 0.4191046 , 0.3294588 , 0.31317067], dtype=float32)},\n",
       " 'alaska': {'topics': array([1889,   66,  308,    9,    9, 1213,  234, 1663,   73, 1194,    9,\n",
       "          583,    9,    9]),\n",
       "  'probabilities': array([0.40441865, 0.30263984, 0.5108042 , 0.4939186 , 0.42890674,\n",
       "         0.38985002, 0.33633626, 0.3866307 , 0.4006964 , 0.5512137 ,\n",
       "         0.39143798, 0.44611776, 0.40020767, 0.40020767], dtype=float32)},\n",
       " 'louisiana': {'topics': array([1712,  436,  436]),\n",
       "  'probabilities': array([0.35896143, 0.45370936, 0.45370936], dtype=float32)},\n",
       " 'massachusetts': {'topics': array([1792, 1272, 1680]),\n",
       "  'probabilities': array([0.34389356, 0.35920346, 0.4701233 ], dtype=float32)},\n",
       " 'vermont': {'topics': array([  73, 1194]),\n",
       "  'probabilities': array([0.4256435 , 0.40929097], dtype=float32)},\n",
       " 'newyork': {'topics': array([ 171,    9,    9,  368,    9,  234,  234, 2023,  402,  402,  402,\n",
       "         1049,  298,  298, 1844, 1239, 1239, 1213, 1413,    9, 1213, 1213,\n",
       "         1213, 1213]),\n",
       "  'probabilities': array([0.325899  , 0.5128206 , 0.5128206 , 0.39755213, 0.39482057,\n",
       "         0.6046109 , 0.6046109 , 0.47240064, 0.4064399 , 0.4064399 ,\n",
       "         0.4064399 , 0.4855569 , 0.33063573, 0.33063573, 0.39536983,\n",
       "         0.4231763 , 0.4231763 , 0.2798318 , 0.38298798, 0.37893283,\n",
       "         0.40834114, 0.40834114, 0.40834114, 0.40834114], dtype=float32)},\n",
       " 'arkansas': {'topics': array([1238, 1238, 1244,  825,  825,  825]),\n",
       "  'probabilities': array([0.495933  , 0.495933  , 0.39482045, 0.34802485, 0.34802485,\n",
       "         0.34802485], dtype=float32)},\n",
       " 'pennsylvania': {'topics': array([ 461, 1194, 1194, 1194, 1194, 1194,    9,    9, 1213, 1213,  114,\n",
       "         1239]),\n",
       "  'probabilities': array([0.54027516, 0.32995638, 0.32995638, 0.32995638, 0.32995638,\n",
       "         0.32995638, 0.4335122 , 0.4335122 , 0.3818736 , 0.3818736 ,\n",
       "         0.4829584 , 0.4033741 ], dtype=float32)},\n",
       " 'alabama': {'topics': array([1272, 1360]),\n",
       "  'probabilities': array([0.4697625, 0.5016178], dtype=float32)},\n",
       " 'kentucky': {'topics': array([ 234,    9,  533, 1703, 1889,    9, 1136]),\n",
       "  'probabilities': array([0.6699451 , 0.48894554, 0.37611642, 0.3831035 , 0.4246846 ,\n",
       "         0.4174749 , 0.39634413], dtype=float32)},\n",
       " 'southcarolina': {'topics': array([622]),\n",
       "  'probabilities': array([0.31094387], dtype=float32)},\n",
       " 'georgia': {'topics': array([ 589,  589,  938, 1912, 1912,  956, 1547,    9]),\n",
       "  'probabilities': array([0.35160512, 0.35160512, 0.500089  , 0.30256993, 0.30256993,\n",
       "         0.38988662, 0.47386682, 0.38994944], dtype=float32)},\n",
       " 'delaware': {'topics': array([858,  81]),\n",
       "  'probabilities': array([0.41915143, 0.37088394], dtype=float32)},\n",
       " 'utah': {'topics': array([   9,    9,    9,    9, 1525, 1525]),\n",
       "  'probabilities': array([0.48962197, 0.48962197, 0.48962197, 0.4226257 , 0.32855856,\n",
       "         0.32855856], dtype=float32)},\n",
       " 'rhodeisland': {'topics': array([903]),\n",
       "  'probabilities': array([0.53065825], dtype=float32)},\n",
       " 'missouri': {'topics': array([1537,  234,  234,  234,  687,  114,  402,    9,  454, 2313, 2313,\n",
       "         2313, 2313, 2313]),\n",
       "  'probabilities': array([0.29989573, 0.5012063 , 0.5012063 , 0.5012063 , 0.4922307 ,\n",
       "         0.5022812 , 0.42077726, 0.4632256 , 0.507585  , 0.4857446 ,\n",
       "         0.4857446 , 0.4857446 , 0.4857446 , 0.4857446 ], dtype=float32)},\n",
       " 'tennessee': {'topics': array([1933,  589, 1889,    9,    9, 1525, 1525, 1525,  439,  439, 1213,\n",
       "         1213]),\n",
       "  'probabilities': array([0.38222027, 0.47969782, 0.402189  , 0.4460554 , 0.4460554 ,\n",
       "         0.41323245, 0.41323245, 0.41323245, 0.25198978, 0.25198978,\n",
       "         0.35353985, 0.35353985], dtype=float32)},\n",
       " 'nebraska': {'topics': array([2070,  424,  723, 1127,   20,  700, 1213, 1213, 1213, 1213, 1213,\n",
       "         1213, 1213]),\n",
       "  'probabilities': array([0.37289286, 0.42005974, 0.4035144 , 0.53034997, 0.44701838,\n",
       "         0.4315011 , 0.58876103, 0.58876103, 0.58876103, 0.58876103,\n",
       "         0.58876103, 0.58876103, 0.58876103], dtype=float32)},\n",
       " 'illinois': {'topics': array([ 234,    9,    9,    9,    9,    9,    9,    9, 1113, 1113,  171,\n",
       "          248, 1032,    9,    9,    9,    9,  424]),\n",
       "  'probabilities': array([0.5931345 , 0.54408467, 0.54408467, 0.54408467, 0.54408467,\n",
       "         0.38450664, 0.38450664, 0.38450664, 0.3545698 , 0.3545698 ,\n",
       "         0.3164887 , 0.43097216, 0.36596507, 0.49126172, 0.49126172,\n",
       "         0.49126172, 0.49126172, 0.3930269 ], dtype=float32)},\n",
       " 'westvirginia': {'topics': array([2070, 1538,  415]),\n",
       "  'probabilities': array([0.47071433, 0.5513009 , 0.36662862], dtype=float32)},\n",
       " 'newmexico': {'topics': array([1070,  875,  875,  875,  622,  473,   22]),\n",
       "  'probabilities': array([0.36083716, 0.5312637 , 0.5312637 , 0.5312637 , 0.38261136,\n",
       "         0.40756643, 0.58560336], dtype=float32)},\n",
       " 'mississippi': {'topics': array([ 533,  770,  194,  194,  194,    9,   89,   89, 1525, 1525, 1525,\n",
       "         1525,  687]),\n",
       "  'probabilities': array([0.4733386 , 0.46439344, 0.3712733 , 0.3712733 , 0.3712733 ,\n",
       "         0.3778885 , 0.4280597 , 0.4280597 , 0.42318526, 0.42318526,\n",
       "         0.42318526, 0.42318526, 0.4464404 ], dtype=float32)},\n",
       " 'kansas': {'topics': array([ 403,    9,    9,    9,  271,  965,  965,  965, 1962, 1962, 1962,\n",
       "         1962,  308, 2187,  114]),\n",
       "  'probabilities': array([0.43383062, 0.3511877 , 0.3511877 , 0.3511877 , 0.36933434,\n",
       "         0.479797  , 0.479797  , 0.479797  , 0.46967548, 0.46967548,\n",
       "         0.46967548, 0.46967548, 0.52692294, 0.30598512, 0.50010115],\n",
       "        dtype=float32)},\n",
       " 'northdakota': {'topics': array([608,  89, 589, 589, 589, 589,   9,   9,   9,   9,   9,   9]),\n",
       "  'probabilities': array([0.40085053, 0.37279803, 0.49617562, 0.49617562, 0.49617562,\n",
       "         0.49617562, 0.5117096 , 0.5117096 , 0.44193792, 0.44193792,\n",
       "         0.44193792, 0.44193792], dtype=float32)},\n",
       " 'idaho': {'topics': array([   9,    9,  314,  830,  234, 1942, 1942, 1889, 1213, 1213]),\n",
       "  'probabilities': array([0.46481556, 0.46481556, 0.5499637 , 0.45845485, 0.5530728 ,\n",
       "         0.42236185, 0.42236185, 0.30262768, 0.3933643 , 0.3933643 ],\n",
       "        dtype=float32)},\n",
       " 'southdakota': {'topics': array([ 542,  542, 1727,    9,    9,  260,  260, 1942,  285, 1213, 1172,\n",
       "         1172, 2331, 2331, 2331, 2331, 2331, 2331,  234,  234,  234,  234,\n",
       "          234,  234,  234,  234,    9,    9,    9,    9,  956,  918,  234,\n",
       "            9,    9, 2023, 1194,    9, 1962, 1962, 1962, 1962, 1962, 1962,\n",
       "         1962,  234]),\n",
       "  'probabilities': array([0.6512569 , 0.6512569 , 0.385269  , 0.48966736, 0.48966736,\n",
       "         0.40835464, 0.40835464, 0.40277976, 0.44681847, 0.28859872,\n",
       "         0.566331  , 0.566331  , 0.34780788, 0.34780788, 0.34780788,\n",
       "         0.34780788, 0.34780788, 0.34780788, 0.6005374 , 0.6005374 ,\n",
       "         0.6005374 , 0.6005374 , 0.6005374 , 0.6005374 , 0.6005374 ,\n",
       "         0.6005374 , 0.48062605, 0.48062605, 0.48062605, 0.48062605,\n",
       "         0.31969088, 0.52022064, 0.54514325, 0.40567297, 0.40567297,\n",
       "         0.47240075, 0.4424438 , 0.44732386, 0.43959463, 0.43959463,\n",
       "         0.43959463, 0.43959463, 0.43959463, 0.43959463, 0.43959463,\n",
       "         0.46700397], dtype=float32)},\n",
       " 'wyoming': {'topics': array([234, 114, 234, 171, 234, 956]),\n",
       "  'probabilities': array([0.6829475 , 0.4440565 , 0.60825455, 0.44969624, 0.64704543,\n",
       "         0.41486   ], dtype=float32)},\n",
       " 'nevada': {'topics': array([   9,    9,    9,    9,    9, 1213,  289,    9, 1962]),\n",
       "  'probabilities': array([0.44628328, 0.44628328, 0.46253628, 0.46253628, 0.46253628,\n",
       "         0.36621058, 0.41603988, 0.43142915, 0.4312636 ], dtype=float32)}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_to_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9896b2cb-4d9e-4677-9bf5-ac050ab52463",
   "metadata": {},
   "outputs": [],
   "source": [
    "political_posts_modeled = dict()\n",
    "\n",
    "for k in state_to_topic.keys():\n",
    "    indices = []\n",
    "    state = state_to_topic[k]\n",
    "    state_topics = state['topics']\n",
    "    state_probs = state['probabilities']\n",
    "\n",
    "    for i in range(len(state_topics)):\n",
    "        if state_topics[i] in political_topic_nums and state_probs[i] >= 0.5:\n",
    "            indices.append(i)\n",
    "    political_posts_modeled[k] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4fb18932-1d32-4fa6-8ca2-40663678114f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'texas': [],\n",
       " 'california': [],\n",
       " 'michigan': [],\n",
       " 'minnesota': [],\n",
       " 'colorado': [],\n",
       " 'wisconsin': [],\n",
       " 'florida': [],\n",
       " 'connecticut': [],\n",
       " 'oregon': [],\n",
       " 'ohio': [],\n",
       " 'northcarolina': [],\n",
       " 'oklahoma': [],\n",
       " 'maryland': [],\n",
       " 'arizona': [],\n",
       " 'virginia': [],\n",
       " 'maine': [],\n",
       " 'indiana': [],\n",
       " 'iowa': [],\n",
       " 'washington': [],\n",
       " 'newhampshire': [],\n",
       " 'alaska': [],\n",
       " 'louisiana': [],\n",
       " 'massachusetts': [],\n",
       " 'vermont': [],\n",
       " 'newyork': [],\n",
       " 'arkansas': [],\n",
       " 'pennsylvania': [],\n",
       " 'alabama': [],\n",
       " 'kentucky': [],\n",
       " 'southcarolina': [],\n",
       " 'georgia': [],\n",
       " 'delaware': [],\n",
       " 'utah': [],\n",
       " 'rhodeisland': [],\n",
       " 'missouri': [],\n",
       " 'tennessee': [],\n",
       " 'nebraska': [],\n",
       " 'illinois': [],\n",
       " 'westvirginia': [],\n",
       " 'newmexico': [],\n",
       " 'mississippi': [],\n",
       " 'kansas': [],\n",
       " 'northdakota': [],\n",
       " 'idaho': [],\n",
       " 'southdakota': [],\n",
       " 'wyoming': [],\n",
       " 'nevada': []}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_posts_modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4652d47-cc23-4cda-b983-d1c9b359d452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
