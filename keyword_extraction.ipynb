{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8842930-7a79-4779-bbb0-2026a5f7abc9",
   "metadata": {},
   "source": [
    "# Extracting Keywords Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d16cdf35-d69f-4bb3-b9af-c2a282aabca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a94e17-2f7d-404b-9ec9-85dea69fa47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for extracting news data using mediastack api. Data has already been extracted and is saved in the newsarticle.json file\n",
    "# Thus, code is commented out\n",
    "\n",
    "import http.client, urllib.parse, json\n",
    "\n",
    "conn = http.client.HTTPConnection('api.mediastack.com')\n",
    "\n",
    "for i in range(0, 6):\n",
    "    params = urllib.parse.urlencode({\n",
    "        'access_key': 'f4578cc2625d968d2ceae7546941a326',\n",
    "        'categories': 'politics',\n",
    "        'countries': 'us',\n",
    "        'sort': 'published_desc',\n",
    "        'limit': 100,\n",
    "        'offset': i*100,\n",
    "        # 'keywords': 'election',\n",
    "        'date': '2024-10-15,2024-11-26'\n",
    "    })\n",
    "\n",
    "    conn.request('GET', '/v1/news?{}'.format(params))\n",
    "    \n",
    "    res = conn.getresponse()\n",
    "    data = res.read().decode('utf-8')\n",
    "    news_data += list(json.loads(data)[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740061ac-decf-4303-8c4c-ab9554051e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write functions for extracting article body from Fox News, The Guardian and Yahoo News\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_yahoo_article(url):\n",
    "    # Step 2: Fetch the webpage HTML\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes (e.g., 404, 500)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        exit()\n",
    "    # Step 3: Parse the HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')  # Or use 'html.parser'\n",
    "    \n",
    "    # Step 4: Extract the article body\n",
    "    # Adjust the class or tag based on the specific structure of the news website\n",
    "    article_body = soup.find_all('p', class_='col-body')\n",
    "    \n",
    "    if article_body:\n",
    "        article_text = \"\"\n",
    "        for lines in article_body:\n",
    "        # Extract and clean the text\n",
    "            article_text += lines.get_text(separator=\"\\n\").strip()\n",
    "            # print(article_text)\n",
    "        return article_text\n",
    "    else:\n",
    "        print(\"Could not find the article body. Please check the class or tag.\")\n",
    "        return None\n",
    "\n",
    "def extract_guardian_article(url):\n",
    "    # Step 2: Fetch the webpage HTML\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes (e.g., 404, 500)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        exit()\n",
    "    # Step 3: Parse the HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')  # Or use 'html.parser'\n",
    "    \n",
    "    # Step 4: Extract the article body\n",
    "    article_body = soup.find('div', id ='maincontent')\n",
    "    \n",
    "    if article_body:\n",
    "        article_text = article_body.get_text(separator=\"\\n\").strip()\n",
    "        return article_text\n",
    "        # print(article_text)\n",
    "    else:\n",
    "        print(\"Could not find the article body. Please check the class or tag.\")\n",
    "        return None\n",
    "\n",
    "def extract_fox_news_article(url):\n",
    "    # Step 2: Fetch the webpage HTML\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes (e.g., 404, 500)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        exit()\n",
    "    # Step 3: Parse the HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')  # Or use 'html.parser'\n",
    "    \n",
    "    # Step 4: Extract the article body\n",
    "    article_body = soup.find('div', class_='article-content')\n",
    "    \n",
    "    if article_body:\n",
    "        article_text = article_body.get_text(separator=\"\\n\").strip()\n",
    "        # print(article_text)\n",
    "        return article_text\n",
    "    else:\n",
    "        print(\"Could not find the article body. Please check the class or tag.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b9331c-2db4-4342-9c4c-028f2b65aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the article body. Please check the class or tag.\n",
      "Could not find the article body. Please check the class or tag.\n",
      "Could not find the article body. Please check the class or tag.\n"
     ]
    }
   ],
   "source": [
    "# Checks for article sources and extracts data from sources that are supported by this script\n",
    "\n",
    "documents = []\n",
    "unsupported_art = []\n",
    "for article in news_data:\n",
    "    url = article[\"url\"]\n",
    "    match article[\"source\"]:\n",
    "        case 'The Guardian':\n",
    "            documents.append(extract_guardian_article(url))\n",
    "        case 'FOX News - Politics':\n",
    "            documents.append(extract_fox_news_article(url))\n",
    "        case 'Yahoo News':\n",
    "            documents.append(extract_yahoo_article(url))\n",
    "        case _:\n",
    "            unsupported_art.append(article)\n",
    "\n",
    "valid_documents = [doc for doc in documents if doc]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35d4bfb-9955-4308-b3c0-4c1f1a300763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_file(data, file_name):\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)  # `indent=4` makes it pretty-printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74b173f-8094-4bd6-bd23-a437d91f588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most related to 'economy':\n",
      "['prices', 'inflation', 'triggering', 'stock', 'scores', 'expensive', 'felt', 'rates', 'respondents', 'professional', 'economic', 'stamps', 'repercussions', 'vampires', 'afloat', 'slowly', 'roxanne', 'exorbitantly', 'unthinkable', 'dandy']\n",
      "Words most related to 'democracy':\n",
      "['reich', 'nausea', 'mudde', 'inbox', 'legalized', 'acquisitions', 'misallocation', 'rectify', 'siphoned', 'eldercare', 'summoned', 'robertreich', 'hence', 'rigs', 'cas', 'newest', 'capitalism', 'columns', 'stalled', 'corporations']\n",
      "Words most related to 'security':\n",
      "['social', 'shortfall', 'insolvency', 'finances', '2031', 'trustees', 'modernize', 'insolvent', 'footing', 'mismanage', 'breck', 'buckle', 'angrily', 'gallup', 'dumas', 'proposing', 'holland', 'oasi', '2033', 'payroll']\n",
      "Words most related to 'immigration':\n",
      "['immigrants', 'policies', 'border', 'forgiveness', 'country', 'about', 'sues', 'migrants', 'illegally', 'wanted', 'harris', 'illegal', 'said', 'did', 'from', 'trump', 'on', 'mass', 'for', 'pressed']\n",
      "Words most related to 'education':\n",
      "['classrooms', 'devos', 'scholarship', 'schreiner', 'blew', 'fordham', 'unaccountable', 'pearson', 'partially', 'betsy', 'petrilli', '529', 'schooling', 'flopped', 'steward', 'maximizing', 'futureed', 'sparsely', 'indoctrination', 'ferial']\n",
      "Words most related to 'healthcare':\n",
      "['blowtorch', 'outpolls', 'schar', '1tn', 'concepts', 'creators', 'uninsured', 'goer', 'obamacare', 'medicaid', 'affordable', 'replacing', 'incorporated', 'ingrained', 'boot', 'subsidies', 'capped', 'empower', 'sections', 'takers']\n",
      "Words most related to 'abortion':\n",
      "['roe', 'bans', 'ban', 'wade', 'abortions', 'life', 'incest', 'exceptions', 'procedure', 'restrictions', 'obtain', 'care', 'pregnancy', 'guaranteeing', 'rape', 'repealed', 'moderates', 'borrowed', 'ducked', 'boulevard']\n"
     ]
    }
   ],
   "source": [
    "# find cosine similarity of relevant categories to the election\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample documents\n",
    "documents = valid_documents\n",
    "\n",
    "# Create a count matrix\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# Create a word-to-index mapping\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Find co-occurrence vectors for a target word\n",
    "keywords = {\"economy\": [], \"democracy\": [], \"security\": [], \"immigration\": [], \"education\": [], \"healthcare\": [], \"abortion\": []}\n",
    "for keyword in keywords:\n",
    "    if keyword in word_to_index:\n",
    "        target_idx = word_to_index[keyword]\n",
    "        target_vector = count_matrix[:, target_idx]\n",
    "    \n",
    "        # Calculate cosine similarity with all other words\n",
    "        similarities = cosine_similarity(target_vector.reshape(1, -1), count_matrix.T).flatten()\n",
    "    \n",
    "        # Get top related words\n",
    "        related_indices = similarities.argsort()[::-1][1:21]\n",
    "        print(f\"Words most related to '{keyword}':\")\n",
    "        for idx in related_indices:\n",
    "            keywords[keyword].append(vocab[idx])\n",
    "        print(keywords[keyword])\n",
    "    else:\n",
    "        print(f\"'{target_word}' not found in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3c22e8-6068-491c-b18e-983c9de612b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "write_file(keywords, \"keywords.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa51ca2-51a1-4477-aa30-2e1f4b74f52d",
   "metadata": {},
   "source": [
    "# End of Script\n",
    "The rest of the script is commented out, and was used to initially extract the data from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c5793-a1b1-4fad-9d09-4ae2e4d727e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_dup = set()\n",
    "# for article in news_data:\n",
    "#     check_dup.add(article[\"url\"])\n",
    "# print(len(check_dup))\n",
    "\n",
    "# check_sources = set()\n",
    "# for article in news_data:\n",
    "#     check_sources.add(article[\"source\"])\n",
    "# print(check_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ced2a9-36b1-4936-8cbe-e663f567d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find Articles from specific sources\n",
    "# articles_by_source = {'CNN': [], 'FOX News': [], 'BBC News': [], 'The Guardian': [], 'CNBC': []}\n",
    "\n",
    "# for article in news_data:\n",
    "#     for source in articles_by_source:\n",
    "#         if source in article[\"source\"]:\n",
    "#             articles_by_source[source].append(article)\n",
    "\n",
    "# for source, article_list in articles_by_source.items():\n",
    "#     print(source + \" has \" + str(len(article_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654be6c-5b52-489a-8fd7-956ae3acef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# # for article in news_data[-5:]:\n",
    "# for article in articles_by_source['The Guardian']:\n",
    "#     print(\"Title: \" + article[\"title\"])\n",
    "#     print(\"Description: \" + article[\"description\"])\n",
    "#     print(\"Source: \" + article[\"source\"])\n",
    "#     print(\"Publish date: \" + article[\"published_at\"] + \"\\n\")\n",
    "#     print(\"URL: \" + article[\"url\"])\n",
    "\n",
    "# # print(data.decode('utf-8'))\n",
    "# # print(type(\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d978dc7-2fc1-489a-a427-42e96acbea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# import pytz\n",
    "# # Check Dates\n",
    "\n",
    "\n",
    "# # Define time cutoffs\n",
    "# # Oct 15th 8pm EST\n",
    "# before_elec_time = datetime(2024, 10, 15, 20, 0, 0, tzinfo=pytz.timezone('America/New_York')).astimezone(pytz.utc).timestamp()\n",
    "# # Nov 26th 8pm EST\n",
    "# after_elec_time = datetime(2024, 11, 26, 20, 0, 0, tzinfo=pytz.timezone('America/New_York')).astimezone(pytz.utc).timestamp()\n",
    "\n",
    "# # Define two ISO 8601 date strings\n",
    "# article_date = \"2024-10-16T19:32:35+00:00\"\n",
    "\n",
    "# # Parse strings into datetime objects\n",
    "# article_date = datetime.fromisoformat(article_date)\n",
    "\n",
    "# # Convert ISO datetime to UTC timestamp\n",
    "# iso_date_timestamp = article_date.timestamp()\n",
    "\n",
    "# print(iso_date_timestamp)\n",
    "\n",
    "# if iso_date_timestamp < before_elec_time:\n",
    "#     print(\"this article was written before our cutoff and should not be considered\")\n",
    "# else:\n",
    "#     print(\"keep this article in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a0b76-48c4-4b64-84a4-e2887733df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract articles\n",
    "# # class=\"col-body\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_yahoo_article(url):\n",
    "#     # Step 2: Fetch the webpage HTML\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()  # Raise an error for bad status codes (e.g., 404, 500)\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"Error fetching the URL: {e}\")\n",
    "#         exit()\n",
    "#     # Step 3: Parse the HTML\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')  # Or use 'html.parser'\n",
    "    \n",
    "#     # Step 4: Extract the article body\n",
    "#     # Adjust the class or tag based on the specific structure of the news website\n",
    "#     article_body = soup.find_all('p', class_='col-body')\n",
    "    \n",
    "#     if article_body:\n",
    "#         article_text = \"\"\n",
    "#         for lines in article_body:\n",
    "#         # Extract and clean the text\n",
    "#             article_text += lines.get_text(separator=\"\\n\").strip()\n",
    "#             # print(article_text)\n",
    "#         return article_text\n",
    "#     else:\n",
    "#         print(\"Could not find the article body. Please check the class or tag.\")\n",
    "#         return None\n",
    "\n",
    "# def extract_guardian_article(url):\n",
    "#     # Step 2: Fetch the webpage HTML\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()  # Raise an error for bad status codes (e.g., 404, 500)\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"Error fetching the URL: {e}\")\n",
    "#         exit()\n",
    "#     # Step 3: Parse the HTML\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')  # Or use 'html.parser'\n",
    "    \n",
    "#     # Step 4: Extract the article body\n",
    "#     article_body = soup.find('div', id ='maincontent')\n",
    "    \n",
    "#     if article_body:\n",
    "#         article_text = article_body.get_text(separator=\"\\n\").strip()\n",
    "#         return article_text\n",
    "#         # print(article_text)\n",
    "#     else:\n",
    "#         print(\"Could not find the article body. Please check the class or tag.\")\n",
    "#         return None\n",
    "\n",
    "# # def extract_new_york_article(url):\n",
    "# #     # Step 2: Fetch the webpage HTML\n",
    "# #     try:\n",
    "# #         response = requests.get(url)\n",
    "# #         response.raise_for_status()  # Raise an error for bad status codes (e.g., 404, 500)\n",
    "# #     except requests.RequestException as e:\n",
    "# #         print(f\"Error fetching the URL: {e}\")\n",
    "# #         exit()\n",
    "# #     # Step 3: Parse the HTML\n",
    "# #     soup = BeautifulSoup(response.text, 'html.parser')  # Or use 'html.parser'\n",
    "    \n",
    "# #     # Step 4: Extract the article body\n",
    "# #     article_body = soup.find('section', name ='articleBody')\n",
    "    \n",
    "# #     if article_body:\n",
    "# #         article_text = article_body.get_text(separator=\"\\n\").strip()\n",
    "# #         print(article_text)\n",
    "# #     else:\n",
    "# #         print(\"Could not find the article body. Please check the class or tag.\")\n",
    "\n",
    "# def extract_fox_news_article(url):\n",
    "#     # Step 2: Fetch the webpage HTML\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()  # Raise an error for bad status codes (e.g., 404, 500)\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"Error fetching the URL: {e}\")\n",
    "#         exit()\n",
    "#     # Step 3: Parse the HTML\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')  # Or use 'html.parser'\n",
    "    \n",
    "#     # Step 4: Extract the article body\n",
    "#     article_body = soup.find('div', class_='article-content')\n",
    "    \n",
    "#     if article_body:\n",
    "#         article_text = article_body.get_text(separator=\"\\n\").strip()\n",
    "#         # print(article_text)\n",
    "#         return article_text\n",
    "#     else:\n",
    "#         print(\"Could not find the article body. Please check the class or tag.\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "# # Step 1: Define the URL\n",
    "# # url = \" https://www.nytimes.com/2024/11/27/us/politics/elon-musk-federal-budget.html\"\n",
    "\n",
    "# # extract_new_york_article(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3705681-fa2a-4528-b22d-2d7c2c0e62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = []\n",
    "# unsupported_art = []\n",
    "# for article in news_data:\n",
    "#     url = article[\"url\"]\n",
    "#     match article[\"source\"]:\n",
    "#         case 'The Guardian':\n",
    "#             documents.append(extract_guardian_article(url))\n",
    "#         case 'FOX News - Politics':\n",
    "#             documents.append(extract_fox_news_article(url))\n",
    "#         case 'Yahoo News':\n",
    "#             documents.append(extract_yahoo_article(url))\n",
    "#         case _:\n",
    "#             unsupported_art.append(article)\n",
    "# print(len(documents))\n",
    "# print(len(unsupported_art))\n",
    "\n",
    "# valid_documents = [doc for doc in documents if doc]\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c1de5-23c6-4af2-bd7c-0ac21c982b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# valid_documents = [doc for doc in documents if doc]\n",
    "# valid_non_support = [doc for doc in unsupported_art if doc]\n",
    "\n",
    "# print(len(valid_documents))\n",
    "# print(len(valid_non_support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14515b6e-cce5-4597-9fc5-0fcdd4aef1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca3e5f-337f-43b1-a53d-7e53e95cf977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ac44e-23c8-43cc-8160-147beba88485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af741c64-d435-418b-9fe4-b914d2f8ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Failed approach: topic modelling\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Download NLTK stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# # Sample data: list of text documents\n",
    "# sample_docs = valid_documents\n",
    "\n",
    "# # Preprocessing the text (removing stop words and punctuation)\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# preprocessed_docs = []\n",
    "# for doc in sample_docs:\n",
    "#     words = word_tokenize(doc.lower())\n",
    "#     words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "#     preprocessed_docs.append(' '.join(words))\n",
    "\n",
    "# # Vectorization using CountVectorizer or TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# doc_term_matrix = vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# # Create the LDA model\n",
    "# num_topics = 9  # Specify the number of topics\n",
    "# lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "# lda.fit(doc_term_matrix)\n",
    "\n",
    "# # Display the topics\n",
    "# for i, topic in enumerate(lda.components_):\n",
    "#     print(f\"Topic {i+1}:\")\n",
    "#     top_words = [vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-30:]]\n",
    "#     print(\"Top words:\", \", \".join(top_words))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642cfef-1d00-4f2d-8e9a-74c00d02bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# election_keywords = lda.components_[8]\n",
    "# top_words = [vectorizer.get_feature_names_out()[index] for index in election_keywords.argsort()[-200:]]\n",
    "# print(\"Top words:\", \", \".join(top_words))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfebbb-f52c-4ed3-949a-fde00c062da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e965ed75-6a2c-41f8-a478-b7a0f9a6ad2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28e962-75ac-4883-a57d-af564dff46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Example list of words\n",
    "# words = top_words\n",
    "\n",
    "# # Load pre-trained Word2Vec model (Google's Word2Vec model is commonly used)\n",
    "# # Note: Download Google's pre-trained Word2Vec from https://code.google.com/archive/p/word2vec/\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b36698-a1db-4cd3-b353-1c4c7aefda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter words to ensure they exist in the model's vocabulary\n",
    "# filtered_words = [word for word in words if word in word_vectors.key_to_index]\n",
    "\n",
    "# # Get word embeddings\n",
    "# embeddings = [word_vectors[word] for word in filtered_words]\n",
    "\n",
    "# # Perform KMeans clustering\n",
    "# num_clusters = 8\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "# clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# # Group words by clusters\n",
    "# clustered_words = {i: [] for i in range(num_clusters)}\n",
    "# for word, cluster in zip(filtered_words, clusters):\n",
    "#     clustered_words[cluster].append(word)\n",
    "\n",
    "# # Display groups\n",
    "# for cluster_id, group in clustered_words.items():\n",
    "#     print(f\"Cluster {cluster_id+1}: {', '.join(group)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bafa7-28d6-43a1-8015-14d719194a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# # Example data (e.g., embeddings from word2vec)\n",
    "# # Replace `data` with your dataset or embeddings\n",
    "# data = embeddings  # Use embeddings or your numeric data\n",
    "\n",
    "# # Calculate inertia for different numbers of clusters\n",
    "# inertia = []\n",
    "# range_clusters = range(1, 20)  # Test cluster numbers 1-10\n",
    "# for k in range_clusters:\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     kmeans.fit(data)\n",
    "#     inertia.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the elbow curve\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(range_clusters, inertia, marker='o')\n",
    "# plt.title(\"Elbow Method for Optimal Clusters\")\n",
    "# plt.xlabel(\"Number of Clusters\")\n",
    "# plt.ylabel(\"Inertia\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e4b43-cc07-4336-8915-6abdc124ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Sample documents\n",
    "# documents = valid_documents\n",
    "\n",
    "# # Create a count matrix\n",
    "# vectorizer = CountVectorizer()\n",
    "# count_matrix = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# # Create a word-to-index mapping\n",
    "# vocab = vectorizer.get_feature_names_out()\n",
    "# word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# # Find co-occurrence vectors for a target word\n",
    "# keywords = {\"economy\": [], \"democracy\": [], \"security\": [], \"immigration\": [], \"education\": [], \"healthcare\": [], \"abortion\": []}\n",
    "# for keyword in keywords:\n",
    "#     if keyword in word_to_index:\n",
    "#         target_idx = word_to_index[keyword]\n",
    "#         target_vector = count_matrix[:, target_idx]\n",
    "    \n",
    "#         # Calculate cosine similarity with all other words\n",
    "#         similarities = cosine_similarity(target_vector.reshape(1, -1), count_matrix.T).flatten()\n",
    "    \n",
    "#         # Get top related words\n",
    "#         related_indices = similarities.argsort()[::-1][1:21]\n",
    "#         print(f\"Words most related to '{keyword}':\")\n",
    "#         for idx in related_indices:\n",
    "#             keywords[keyword].append(vocab[idx])\n",
    "#             # print(f\"{vocab[idx]}: {similarities[idx]:.2f}\")\n",
    "#         # print(keyword + \": \")\n",
    "#         print(keywords[keyword])\n",
    "#     else:\n",
    "#         print(f\"'{target_word}' not found in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72776e99-1f71-409d-9ff9-1e77df6cf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save dictionary to JSON file\n",
    "# with open(\"newsarticles.json\", \"w\") as json_file:\n",
    "#     json.dump(documents, json_file, indent=4)  # `indent=4` makes it pretty-printed\n",
    "\n",
    "# print(\"Dictionary saved to newsarticles.json!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cdd53d-adf4-46d9-aa74-0ede3a5e6243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
